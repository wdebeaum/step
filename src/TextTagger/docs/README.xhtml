<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<?xml-stylesheet type="text/xsl" href="toc.xsl"?>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>TextTagger</title>
<link href="style.css" rel="stylesheet" type="text/css" />
</head><body>

<div>

<div class="section">
<h2>Introduction</h2>
<p>TextTagger is a module in the TRIPS system which finds "tags" in text
strings and sends them to the parser. Tags associate various information with
spans of characters in the input string. For example, this information can
include:
<ul>
<li>that the span is punctuation/a word/a comma-separated clause/a sentence</li>
<li>the part of speech of the word</li>
<li>that the span names some kind of entity</li>
</ul>
</p>
</div>

<div class="section">
<h2>Installation</h2>
<p>TextTagger glues together several different taggers, including several
written in Perl with TextTagger itself,
<a href="http://nlp.stanford.edu/">Stanford</a>'s
<a href="http://nlp.stanford.edu/software/corenlp.shtml">CoreNLP</a>
(as well as the older separate parts thereof,
the <a href="http://nlp.stanford.edu/software/CRF-NER.shtml">NER</a>,
the <a href="http://nlp.stanford.edu/software/tagger.shtml">POS</a> tagger, and
the <a href="http://nlp.stanford.edu/software/lex-parser.shtml">parser</a>),
the <a href="http://www.nactem.ac.uk/enju/">Enju parser</a>,
Charniak and Johnson's 
<a href="http://bllip.cs.brown.edu/resources.shtml">parser</a>
(using McClosky's
<a href="http://www.cs.brown.edu/~dmcc/biomedical.html">self-trained biomedical model</a>) and
<a href="http://www.nlm.nih.gov/research/umls/">UMLS</a>'
<a href="http://mmtx.nlm.nih.gov/">MetaMap</a>.
Some taggers require resources that you must download and install if you want
to use those taggers (if you don't, TextTagger should still work otherwise). If
you're on the URCS network, it's taken care of for you (except for MetaMap, 
CJParser, and Enju, but most TextTagger users don't need those); the resources
are installed in <code>/p/nl/</code>. If you're not, then you have to install
them in either <code>/p/nl/</code> or <code>/usr/local/share/</code>, or you
have to specify where they're installed using the appropriate configure
options.</p>

<p>How to install the necessary resources in <code>/usr/local/share/</code>:
<ul>
<li>Get the Stanford tools. To use the integrated CoreNLP version:
 <ul><li>Download <a href="http://nlp.stanford.edu/software/stanford-corenlp-full-2016-10-31.zip">this Stanford CoreNLP package</a> and unpack it in <code>/usr/local/share/stanford-corenlp/</code> (create the directory if necessary).</li></ul>
 To use the old separate tools:
 <ul>
  <li>Download <a href="http://nlp.stanford.edu/software/stanford-ner-2007-11-05.tar.gz">this Stanford NER package</a> and unpack it in <code>/usr/local/share/stanford-ner/</code>.</li>

  <li>Do the same for <a href="http://nlp.stanford.edu/software/stanford-postagger-2008-06-06.tar.gz">this Stanford POS Tagger package</a> and <code>/usr/local/share/stanford-postagger/</code>.</li>

  <li>...and <a href="http://nlp.stanford.edu/software/stanford-parser-2007-08-19.tar.gz">this Stanford Parser package</a> and <code>/usr/local/share/stanford-parser/</code>.</li>
 </ul>
</li>

<li>To use the Enju parser, <a href="http://www.nactem.ac.uk/enju/#download">download</a> the binary package for your platform (you will have to fill out a form), and unpack it in <code>/usr/local/share/enju/</code>.</li>

<li>To use the Charniak-Johnson parser with McClosky's biomedical model:
 <ul>
  <li>Download the parser. Which version of the parser you get depends on your OS:
   <dl>
    <dt>older versions of Mac OS X and Linux</dt>
    <dd>Download <a href="http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz">this Charniak-Johnson Parser package</a> and unpack it in <code>/usr/local/share/cj-parser/</code>.</dd>

    <dt>Mac OS X 10.10 Yosemite (possibly also 10.9, see below)</dt>
    <dd>
     <ul>
      <li>Change to the right directory: <code>cd /usr/local/share/cj-parser</code></li>
      <li>Download the repo: <code>git clone --depth 1 https://github.com/jimwhite/bllip-parser.git</code>
       <ul>
	<li>Note that this is a <a href="https://github.com/jimwhite/bllip-parser">fork</a> from <a href="https://github.com/BLLIP/bllip-parser">the official BLLIP parser</a> (which is what they're calling the Charniak-Johnson parser nowadays). It was created in response to the fact that <a href="http://github.com/BLLIP/bllip-parser/issues/19">the official version can't be compiled on Mac OS X 10.9+</a>. It might not work on other platforms.</li>
	<li>Also note that the <code>--depth 1</code> option causes <code>git</code> to retrieve only the latest revision in the repository, as opposed to its default behavior of retrieving the entire history, which in this case is reported to be a burdensome download.</li>
       </ul>
      </li>
      <li>Make a symlink to make it look like you unpacked the old package: <code>ln -s bllip-parser reranking-parser</code></li>
      <li>Add <code>#include &lt;unistd.h&gt;</code> after <code>#include &lt;vector&gt;</code> in <code>second-stage/programs/features/best-parses.cc</code> (I guess Jim White missed this bit?)</li>
      <li>Install gcc 4.9 from MacPorts (XCode's "g++" program actually runs Clang instead, which doesn't work for this parser): <code>sudo port install gcc49</code></li>
     </ul>
    </dd>
    
    <dt>newer versions of Linux, possibly others</dt>
    <dd>
     <ul>
      <li>Change to the right directory: <code>cd /usr/local/share/cj-parser</code></li>
      <li>Download the repo: <code>git clone --depth 1 https://github.com/BLLIP/bllip-parser.git</code> (see the note about <code>--depth 1</code> above)</li>
      <li>Make a symlink to make it look like you unpacked the old package: <code>ln -s bllip-parser reranking-parser</code></li>
     </ul>
    </dd>
   </dl>
  </li>

  <li>Run <code>make clean</code> in <code>/usr/local/share/cj-parser/reranking-parser/second-stage/programs/</code> (they left some compiled Linux binaries in there that interfere with the next step).</li>

  <li>Run <code>make</code> in <code>/usr/local/share/cj-parser/reranking-parser/</code>. Again this depends on your OS:
   <dl>
    <dt>Mac OS X &lt; 10.6, Linux, possibly others</dt>
    <dd>Just run <code>make</code>.</dd>

    <dt>Mac OS X 10.6-10.8?</dt>
    <dd>Run <code>make GCCFLAGS="-arch i386" LDFLAGS="-arch i386"</code> instead of just <code>make</code>, in order to compile a 32-bit binary and avoid problems compiling this code in 64-bit mode (which 10.6 does by default).</dd>
    
    <dt>Mac OS X 10.9</dt>
    <dd>I'm not exactly sure where the transition between the 10.6 way and the 10.10 way happened. I have a 10.9 machine that works in the 10.6 way, but the BLLIP bug report says 10.9 doesn't work that way. Try one and then the other, and <a href="mailto:wbeaumont@ihmc.us">let me know what worked</a>.</dd>
    
    <dt>Mac OS X 10.10+ (with gcc 4.9 from MacPorts as suggested above)</dt>
    <dd>Run <code>make CXX=g++-mp-4.9 GCCFLAGS="-ansi" LDFLAGS="-L/opt/local/lib"</code> instead of just <code>make</code>, in order to use gcc 4.9 instead of Clang.</dd>
   </dl>
  </li>

  <li>Download <a href="http://bllip.cs.brown.edu/download/bioparsingmodel-rel1.tar.gz">this biomedical model package</a> and unpack it in <code>/usr/local/share/cj-parser/</code>.</li>
 </ul>
</li>

<li>Download
<a href="http://geonames.usgs.gov/docs/stategaz/NationalFile_20160201.zip">this
GNIS geographic names database package</a> and put it in
<code>/usr/local/share/geonames/2016-02-01/</code>. The
name of this file appears to change frequently. If that link doesn't work, go
to the <a href="http://geonames.usgs.gov/">geonames</a> site and select
"domestic names" and then "download" from the navigation menu. On that page
there should be a link to a zip file containing one text file for the whole
nation (not individual state files). Rename the zip file to
<code>NationalFile.zip</code> and put it in a directory named for the date on
the file, with dashes. No need to unpack it.</li>

<li>Running MetaMap locally requires approximately 28GB of disk space (though the 16GB of UMLS data files could be deleted after installation), and some extra installation effort, so by default a remote MetaMap server is used instead. The server already has all of these UMLS resources installed, so you won't have to install them in order to use MetaMap. If you want to use MetaMap locally:
<ul>
<li>Install <a href="http://www.mysql.com/">MySQL</a> (the free "Community Edition" is fine), and make sure you have the location of the <code>mysql_config</code> program in your <code>$PATH</code> (e.g. <code>/usr/local/mysql/bin/</code>).</li>

<li>Install the <code>DBD::mysql</code> Perl module. You can do this using CPAN like so: <code>sudo cpan DBD::mysql</code>.</li>

<li>Obtain a license code for UMLS and follow their instructions for installing it. You should install it so that you have the directory <code>/usr/local/share/UMLS/2010AB/</code> in that location. If the <code>2010AB</code> part is different (e.g. <code>2009AA</code>) leave it the way it is and instead give this argument to <code>configure</code>: <code>--with-umls=/usr/local/share/UMLS/2009AA/</code> (using the previous example).</li>

<li>Download the semantic groups file from <a href="http://semanticnetwork.nlm.nih.gov/SemGroups/">here</a> (it should be called SemGroups.txt) and put it in <code>/usr/local/share/UMLS/</code> (this one doesn't require a license code).</li>

<li>Download <a href="http://mmtx.nlm.nih.gov/">MetaMap</a> (not MMTx) and install it in <code>/usr/local/share/UMLS/</code>. You should then have the directory <code>/usr/local/share/UMLS/public_mm/bin/</code> (which contains the programs TextTagger needs to run).
</li>

<li>After running <code>configure</code> and starting MySQL, install the necessary UMLS tables in the MySQL database by running <code>make install-umls-mysql-db</code> in <code>src/TextTagger/</code> (after this step the UMLS data files in <code>/usr/local/share/UMLS/</code> are no longer needed and may be deleted to save space).</li>

<li>When running TextTagger, pass the <code><a href="#sec-3.1.6.">-remote-meta-map</a> no</code> option (passing <code>-metamap-server no</code> to <code>trips-cernl</code> does this too).</li>
</ul>
</li>

<li>To use WordNet, follow the instructions in <code>src/WordFinder/README.txt</code>. TextTagger will use the WordFinder configuration to find the necessary WordNet files.</li>

<li>The following Perl modules are also required to build one of the data files for the Drum tagger (<code>go_protmods.tsv</code>): Getopt::Long, Set::Scalar, File::Slurp. If you're using MacPorts, you can install the corresponding packages by downcasing the module name, turning <code>::</code> into <code>-</code> and prepending <code>p5-</code>: <code>sudo port install p5-getopt-long p5-set-scalar p5-file-slurp</code>.</li>

<li>Note that the TextTagger Makefile also downloads and processes some files for you, depending on what parts of it are included in your checkout.</li>

</ul>
</p>

<p>TextTagger also requires the following dependencies (you might already have them, though):</p>
<ul>
<li><a href="http://www.perl.org/">Perl</a> &gt;= 5.6?</li>
<li><a href="http://www.gnu.org/software/make/">GNU Make</a> (other versions of make might work too)</li>
<li><a href="http://www.gnu.org/software/bash/">Bash</a></li>
<li>Other common UNIX tools like cut, sort, uniq, iconv, unzip, etc.</li>
<li>Other parts of TRIPS, including KQML, TripsModule, and the Facilitator. TextTagger also makes symlinks to a couple of Perl libraries that live in TextPP.</li>
</ul>

<p>And it often requires these common dependencies, depending on which taggers are included:</p>
<ul>
<li><a href="http://xmlsoft.org/XSLT/xsltproc2.html">xsltproc</a> (used to process the XML output of MetaMap and StanfordCoreNLP)</li>
<li><a href="http://www.java.com/">Java</a> &gt;= 1.5 (used for the Stanford components)</li>
<li>A C/C++ compiler (e.g. <a href="http://gcc.gnu.org">GCC</a>) (used for compiling terms.c and/or terms2.cpp, which are used by Asma, Countries, Drum, NamesFromFile, PersonalNames, PlaceNames, PseudoMetaMap, Specialist, TermsFromFile, Terms, and WordNet)</li>
<li>The <a href="http://site.icu-project.org/home">ICU</a> C++ library (used in terms2.cpp, which is used by Countries, Drum, PlaceNames, Specialist, TermsFromFile; in this case your C++ compiler must support the C++11 standard, specifically the <code>char16_t</code> type)</li>
<li><a href="https://www.freedesktop.org/wiki/Software/pkg-config/">pkg-config</a> (used to find ICU)</li>
<li><a href="http://aspell.net/">GNU Aspell</a> (used for the Misspellings tagger)</li>
</ul>

</div>

<div class="section">
<h2>Running</h2>

<div class="section">
<h3>Command-line arguments</h3>
<p>In addition to the standard TripsModule command-line arguments, TextTagger accepts the following:</p>

<div class="section">
<h4>-config-file</h4>
<p><code>-config-file <var>filename</var></code></p>
<p>Load a configuration file containing command-line arguments to insert at this point in the list of arguments. The file may contain newlines (treated as spaces) and comments starting with <code>#</code> and continuing to the end of the line. Environment variables may be referenced with a <code>$</code> prefix. Whitespace within arguments and literal <code>$</code>s may be escaped with a preceding backslash, and all backslashes will be removed. Other shell characters (other backslash escapes, quotes, pipes, etc.) are not interpreted.</p>
</div>

<div class="section">
<h4>-init-taggers</h4>
<p><code>-init-taggers <var>"all" | comma-separated list of taggers</var></code></p>
<p>Select which of the initializable taggers to initialize. The <a
href="#sec-5.2.">table of taggers</a> has a column indicating which taggers are
initializable. These taggers use the external resources listed above.
<code>-init-taggers</code> must be followed by either <code>all</code>
(indicating that all initializable taggers are to be initialized), or a
comma-separated list of taggers to initialize (in their <code>:type</code>
form, see below). By default, no taggers are initialized (but in that case you
can still use those taggers that don't require initialization). If this
argument is given more than once, TextTagger takes the union of the sets of
taggers to initialize.</p>
</div>

<div class="section">
<h4>-dont-init-taggers</h4>
<p><code>-dont-init-taggers <var>"all" | comma-separated list of taggers</var></code></p>
<p>Undo the effect of a previous <code>-init-taggers</code> argument for the given taggers. That is, select taggers <em>not</em> to be initialized.</p>
</div>

<div class="section">
<h4>-default-type</h4>
<p><code>-default-type <var>tag type expression</var></code></p>
<p>Specify the tag type to be used when no <code>:type</code> argument is given, or when the special type <code>default</code> is used in another tag type expression. If this argument is given more than once, <code>default</code> in the later arguments will refer to the previous arguments. The default value of <code>default</code> is:
  <blockquote>
    <code>(or terms words punctuation named_entities street_addresses capitalized_names alphanumerics quotations alternate_spellings (and stanford_pos pos))</code>
  </blockquote>
Note that in order to get around the limitations of some shells, it's OK to use <code>[]</code> instead of <code>()</code> here.
</p>
</div>

<div class="section">
<h4>-process-input-utterances</h4>
<p><code>-process-input-utterances <var>boolean</var></code> (default false)</p>
<p>When true, TextTagger will subscribe to messages of the form <code>(tell
&amp;key :content (utterance . *))</code> and translate them to tag requests
internally. These will be of the form <code>(request :content (tag :text
<var>text</var> :tell-each-tag t :extra-tag-args (<var>args</var>)))</code>,
where <code><var>text</var></code> comes from the <code>:text</code> argument
of the <code>utterance</code>, and <code><var>args</var></code> come from the
<code>:channel</code>, <code>:direction</code>, <code>:mode</code>,
<code>:genre</code>, and <code>:uttnum</code> arguments. The original utterance
message is also re-sent with the <code>:sender</code> removed, to indicate the
end of the output. A <code>start-new-sentence</code> message with the same
<code><var>args</var></code> is sent before the internal tag request is
processed in order to mark the beginning of the output from TextTagger.  (Note
that when <code>utterance</code>s are sent in rapid succession, relying on
<code>started/stopped-speaking</code> messages from the sender of the original
<code>utterance</code> is error-prone because you might get a
<code>started-speaking</code> for the next utterance/sentence from the original
sender before the <code>utterance</code> for the previous utterance from
TextTagger. See the section on <a href="#sec-4.1.3.">splitting utterances</a>.)</p>
</div>

<div class="section">
<h4>-remote-meta-map</h4>
<p><code>-remote-meta-map <var>boolean|HOST:PORT</var></code> (default trips.ihmc.us:6300)</p>
<p>Specify whether to connect to a remote server (yes) or run MetaMap locally (no). You can also specify the host and port of the server to connect to.</p>
<p>MetaMap (the program used by the MetaMap tagger) and UMLS (the database it refers to) require a lot of disk space and installation effort, so most people using the MetaMap tagger will want to use the server. See also <a href="#sec-3.7.">Running TextTagger remotely</a> (the old way of doing it).</p>
</div>

<div class="section">
<h4>-meta-map-sources</h4>
<p><code>-meta-map-sources <var>"all" | comma-separated list of UMLS source vocabularies</var></code> (default all)</p>
<p>Specify which source vocabularies we should accept tags from. Any vocabularies not in this list will be stripped from the <code>:sources</code> argument of the <code>:domain-specific-info</code> argument of each tag from MetaMap. Any tags whose sources have all been removed are themselves removed. "all" means we should keep all the sources. There is a list of source vocabularies in <code>Perl/TextTagger.pm</code> called <code>@all_meta_map_sources</code>. See also the per-request version, <a href="#sec-3.5.15.">:meta-map-sources</a>.</p>
</div>

<div class="section">
<h4>-drum-species</h4>
<p><code>-drum-species <var>"all" | comma-separated list of species names</var></code> (default all)</p>
<p>Specify which species we should accept UniProt IDs from in the Drum tagger (also affects miRNAs). Recognised species are "Homo sapiens"/"Human", "Mus musculus"/"Mouse", "Arabidopsis"/"Cress", "Oryza"/"Rice", and "Hordeum"/"Barley" (the last three are really genuses, oh well). See also the per-request version, <a href="#sec-3.5.16.">:drum-species</a>.</p>
</div>

<div class="section">
<h4>-no-sense-words</h4>
<p><code>-no-sense-words <var>comma-separated list of words</var></code> (default empty list)</p>
<p>Specify words for which TextTagger should not send sense information. This is used in CERNL to filter out bogus senses of common words that come from MetaMap.</p>
</div>

<div class="section">
<h4>-senses-only-for-penn-poss</h4>
<p><code>-senses-only-for-penn-poss <var>"all" | comma-separated list of Penn parts of speech</var></code> (default all the POSs)</p>
<p>Specify Penn POSs for which TextTagger should send sense information beyond the POS itself (that is, <code>:ont-types</code>, <code>:wn-sense-keys</code>, and <code>:domain-specific-info</code>). This is useful for focusing on disambiguating senses of specific parts of speech and removing bad results from others. Note that this option only works for <a href="#sec-4.1.">lattice format</a> and not <a href="#sec-4.2.">native format</a>, because the tags need to be combined in order to know the POS for all the sense tags.</p>
</div>

<div class="section">
<h4>-names-file</h4>
<p><code>-names-file <var>filename</var></code></p>
<p>Specify the file full of names that the NamesFromFile tagger should tag as named entities. Note that setting this does not automatically (re)initialize the NamesFromFile tagger or add it to the default tag type; you do that yourself.</p>
</div>

<div class="section">
<h4>-terms-file</h4>
<p><code>-terms-file <var>filename</var></code></p>
<p>Specify the TSV file mapping terms to native tags that the TermsFromFile should use to tag them. This file is similar to the <a href="#sec-3.6.">input terms list</a> for the TermsInput tagger, but they are separate. You can use the <code>input-terms-to-terms-file.pl</code> program to convert an input terms list to a TSV file. Note that setting this does not automatically (re)initialize the TermsFromFile tagger or add it to the default tag type; you do that yourself.</p>
</div>

<div class="section">
<h4>-xml-tags</h4>
<p><code>-xml-tags <var>mode</var></code></p>
<p>Specify how to treat XML tags and entities (things like <code>&amp;...;</code>); <code><var>mode</var></code> should be one of the following:
<dl>
<dt><code>keep</code></dt>
<dd>Keep XML tags and entities in the text, don't do anything to them. This is the default.</dd>
<dt><code>remove</code></dt>
<dd>Remove XML tags from the text and replace XML entities with the characters they represent (the few TT recognizes anyway).</dd>
<dt><code>replace-with-spaces</code></dt>
<dd>Replace XML tags in the text with a string of spaces of the same length, and replace XML entities with the characters they represent followed by spaces, so that character offsets into the original string are preserved. Note that as a special case, the Sentences tagger will act as if the mode were <code>remove</code>, removing the extra spaces before sending the string to Lingua::EN::Sentence. It puts them back afterwards. This special case was added so that the tagger wouldn't be confused by spaces inserted between the last word in a sentence and its final punctuation.</dd>
</dl>
</p>
</div>

<div class="section">
<h4>-xml-input-rules-file</h4>
<p><code>-xml-input-rules-file <var>filename</var></code></p>
<p>Specify the file full of rules for the XMLInput tagger to use to translate XML tags in the input text to TextTagger tags. The file may also contain comments starting with either <code>#</code> or <code>;</code> and continuing to the end of the line. Rules consist of a left-hand side (LHS) describing XML tags that match, and a right-hand side (RHS) describing the (<a href="#sec-4.2.">native-format</a>) TextTagger tag(s) that the XMLInput tagger should output when there is a match (if more than one tag is on the RHS they must be enclosed in another set of parens). Rules may contain newlines, but the LHS must start at the beginning of a line. The LHS and RHS are separated by <code>-&gt;</code>. For example, this rule says that any XML tags with the tag name <code>N</code> should result in a <code>pos</code> tag whose <code>:penn-pos</code> is <code>(NN NNS)</code>:</p>
<blockquote><code>
&lt;N&gt; -&gt; (pos :penn-pos (NN NNS))
</code></blockquote>
<p>The LHS may also specify XML attributes, with or without values. If a value is specified (e.g. <code>&lt;N number="plural"&gt;</code>), it must match the value of the same attribute in the input. If no value is specified (e.g. <code>&lt;N number&gt;</code>), any value matches, as long as the attribute is there at all.</p>
<p>Variables are indicated with a <code>?</code> prefix, and may be used for the XML tag name or attribute values (but not attribute names) on the LHS, and may be used anywhere outside quotes on the RHS, or as the only thing inside quotes (e.g. <code>... :text "?text" ...</code>). Attributes with no values implicitly bind a variable named the same as the attribute, e.g. <code>&lt;N number&gt;</code> becomes <code>&lt;N number="?number"&gt;</code>. There are a number of other implicitly-defined variables for all rules (these may be overridden by using them explicitly in the LHS):</p>
<dl>
<dt><code>?lex</code> and <code>?text</code></dt>
<dd>Both contain the content of the XML element without the tags, and with whitespace trimmed from both ends.</dd>
<dt><code>?start</code> and <code>?end</code></dt>
<dd>These are the start and end character offsets of <code>?lex</code>/<code>?text</code> within the input text.</dd>
<dt><code>?tag-start</code> and <code>?tag-end</code></dt>
<dd>These are the start and end character offsets of the whole XML element, including the XML tags, within the input text.</dd>
<dt><code>?content-start</code> and <code>?content-end</code></dt>
<dd>These are the start and end character offsets of the contents of the XML element, before whitespace is trimmed, within the input text.</dd>
</dl>
<p>By default, tags on the RHS implicitly have the arguments <code>:lex "?lex" :text "?text" :start ?start :end ?end</code> appended to them, but they can be overridden by explicitly specifying different values for these arguments. One of <code>:lex</code> or <code>:text</code> is sufficient to override both.</p>
<p><code>case</code> expressions may appear anywhere on the RHS. They are similar to <code>ecase</code> in Common Lisp, and <code>switch</code> in C-family languages. For example, this rule:</p>
<blockquote><pre>
&lt;N number&gt; -&gt; (pos :penn-pos (
  (case ?number
    (singular NN)
    (plural NNS)
    )))
</pre></blockquote>
<p>says that any XML element whose tag name is <code>N</code> and that has a <code>number</code> attribute turns into a <code>pos</code> tag with a <code>:penn-pos</code> argument, which is a list with one item in it. The value of that list item depends on the value of the <code>number</code> attribute. If the attribute value is <code>singular</code>, the list item will be <code>NN</code>. If the attribute is <code>plural</code>, the list item will be <code>NNS</code>. If it's anything else, an error occurs (this is why it's more similar to Lisp's <code>ecase</code> than <code>case</code>).</p>
</div>

<div class="section">
<h4>-parsers-must-agree</h4>
<p><code>-parsers-must-agree <var>boolean</var></code> (default false)</p>
<p>When true, only the phrase categories (<code>:penn-cats</code>/<code>:trips-cats</code>) that all enabled parsers agree on for a given span will be output. Otherwise, all phrase categories that any parser outputs for that span will be output. Setting this to true is necessary when the TRIPS parser's barrier penalty is used, because conflicting phrase tags confuse it.</p>
</div>

<div class="section">
<h4>-use-wordfinder</h4>
<p><code>-use-wordfinder <var>boolean</var></code> (default true)</p>
<p>When true, TextTagger may instruct LexiconManager to use WordFinder when checking whether a word is in the TRIPS lexicon. When false, WordFinder is never used for such checks. Some checks never use WordFinder in the first place. See also the per-request version, <a href="#sec-3.5.17.">:use-wordfinder</a>.</p>
</div>

<div class="section">
<h4>-min-sentence-length-for-phrases</h4>
<p><code>-min-sentence-length-for-phrases <var>nonnegative-integer</var></code> (default 0)</p>
<p>Specify the minimum length of a sentence for which TextTagger will output phrase tags (<code>prefer</code> messages) within the sentence. Sentence length is measured in number of tokens (contiguous strings of non-space characters). If there are no sentence tags, the whole input is taken to be one sentence. Note that sentence tags themselves may still generate <code>prefer</code> messages.</p>
</div>

<div class="section">
<h4>-aspell-dict</h4>
<p><code>-aspell-dict <var>filename</var></code></p>
<p>Specify the dictionary file that the Misspellings tagger will pass to <a href="http://aspell.net/">GNU Aspell</a> in its <code>--master</code> argument. By default the argument isn't passed, so aspell will use its default dictionary.</p>
</div>

</div>

<div class="section">
<h3>The set- and get-parameters messages</h3>
<p>Some of TextTagger's command-line arguments can be changed at runtime using the <code>set-parameters</code> request. For example:</p>
<blockquote><samp>
(request :receiver TextTagger :content
  (set-parameters
    :default-type (or words punctuation meta-map)
    :process-input-utterances nil
    :meta-map-sources (SNOMEDCT)
    :no-sense-words (history procedure)
    :senses-only-for-penn-poss (NN NNS NNP NNPS)
    )
  :sender test
  :reply-with test123)
(reply :sender TextTagger :receiver test :in-reply-to test123 :content (ok))
</samp></blockquote>
<p>TextTagger does not subscribe for this request, so you must specify <code>:receiver TextTagger</code> explicitly (other modules might have their own parameters to set). <code>:default-type</code>, <code>:process-input-utterances</code>, <code>:meta-map-sources</code>, <code>:drum-species</code>, <code>:no-sense-words</code>, <code>:senses-only-for-penn-poss</code>, <code>:xml-tags</code>, <code>:parsers-must-agree</code>, <code>:use-wordfinder</code>, and <code>:min-sentence-length-for-phrases</code> can be set at runtime. <code>-init-taggers</code> cannot be set using <code>set-parameters</code>; use <code>init</code> and <code>fini</code> messages for that (see <a href="#sec-3.3.">below</a>). <code>:remote-meta-map</code>, <code>:names-file</code>, <code>:terms-file</code>, <code>:xml-input-rules-file</code>, and <code>:aspell-dict</code> can be set, but don't take effect until the related taggers (meta-map, names-from-file, terms-from-file, xml-input, and misspellings, respectively) are re-initialized (using <code>re-init</code> or <code>fini</code> and <code>init</code>).</p>
<p>You can also get the current runtime values of all parameters (including the currently initialized taggers) using the <code>get-parameters</code> request, for example:</p>
<blockquote><samp>
(request :receiver TextTagger :content (get-parameters) :sender test :reply-with test456)
(reply :sender TextTagger :receiver test :in-reply-to test456 :content
  (parameters
    :init-taggers (meta-map)
    :default-type (or words punctuation meta-map)
    :process-input-utterances nil
    :remote-meta-map "trips.ihmc.us:6300"
    :meta-map-sources (SNOMEDCT)
    :drum-species all
    :no-sense-words (history procedure)
    :min-sentence-length-for-phrases 0
    :names-file nil
    :terms-file nil
    :xml-tags keep
    :xml-input-rules-file nil
    :parsers-must-agree nil
    :use-wordfinder t
    :aspell-dict nil
    ))
</samp></blockquote>
</div>

<div class="section">
<h3>The init, fini, and re-init messages</h3>
<p>Individual taggers can be initialized, finalized, and re-initialized using the <code>init</code>, <code>fini</code>, and <code>re-init</code> requests, respectively. For example:</p>
<blockquote><samp>
;; initialize the meta-map tagger
(request
  :receiver TextTagger
  :content (init :taggers (meta-map))
  :sender test
  :reply-with test123)
(reply :sender TextTagger :receiver test :in-reply-to test123 :content (ok))

;; finalize the stanford-pos and stanford-parser taggers
(request
  :receiver TextTagger
  :content (fini :taggers (stanford-pos stanford-parser))
  :sender test
  :reply-with test456)
(reply :sender TextTagger :receiver test :in-reply-to test456 :content (ok))

;; re-initialize the meta-map tagger
(request
  :receiver TextTagger
  :content (re-init :taggers (meta-map))
  :sender test
  :reply-with test789)
(reply :sender TextTagger :receiver test :in-reply-to test789 :content (ok))
</samp></blockquote>
<p><code>init</code> and <code>fini</code> are useful for changing which taggers are initialized (effectively changing the <code>-init-taggers</code> parameter, see <a href="#sec-3.2.">above</a>), while <code>re-init</code> is useful for restarting taggers that have stopped working, or have had a configuration change. Note that <code>re-init</code> won't work for hung taggers, because TextTagger won't be able to process the message while waiting for the tagger.</p>
</div>

<div class="section">
<h3>The utterance message</h3>
<p>The main way TextTagger operates is by reacting to utterance messages like this one:</p>
<blockquote><samp>
(tell :sender keyboardmanager :content
  (utterance
    :text "Hello, world!"
    :channel Desktop
    :direction input
    :mode text
    :genre text
    :uttnum 1))
</samp></blockquote>
<p>It does so by converting them to tag requests internally. See the section on the <a href="#sec-3.1.5.">-process-input-utterances</a> argument for how this conversion works, and the section on <a href="#sec-3.5.">the tag request</a> for explanations of the tag request arguments.</p>
</div>

<div class="section">
<h3>The tag request</h3>
<p>Here is an example of the <code>tag</code> request, along with TextTagger's reply:</p>
<blockquote><samp>
(request :content (tag :text "Hello, world!"))
(reply :content (
  (word "Hello" :frame (0 4) :sense-info ((
    :penn-parts-of-speech (NNP NNPS)
    :trips-parts-of-speech (W::NAME)
    :ont-types (ONT::REFERENTIAL-SEM)
    )))
  (word "," :frame (5 6))
  (word "world" :frame (7 11))
  (word "!" :frame (12 12))
  ))
</samp></blockquote>

<p>A description of each argument follows.</p>

<div class="section">
<h4>:text</h4>
<p><code>:text <var>string</var></code></p>
<p>The string to tag. This is the only required argument; the rest are optional.</p>
</div>

<div class="section">
<h4>:type</h4>
<p><code>:type <var>tagger/tag type expression</var></code></p>
<p>Select which taggers to use for this text, and which tag types to return.
This is structured as a Lisp-like boolean expression, which is applied to
each tagger to decide whether to run it, and to each tag to decide whether to
output it.</p>

<dl>  
<dt><code>(and ...)</code>, <code>(or ...)</code>, <code>(not <var>expression</var>)</code></dt>
  <dd>These do what you think.</dd>
<dt><code>(type <var>type-name</var>)</code></dt>
  <dd>A tagger matches this if it can produce tags of the given type. A tag matches this if it is of the given type.</dd>
<dt><code>(source <var>tagger-name</var>)</code></dt>
  <dd>A tagger matches this if it has the given name. A tag matches this if it was produced by the named tagger.</dd>
<dt><code><var>name</var></code></dt>
  <dd>Shorthand for <code>(or (type <var>name</var>) (source <var>name</var>))</code>. Note that type names are generally singular and tagger names are generally plural, so you can usually avoid using the explicit <code>(type <var>type-name</var>)</code> and <code>(source <var>tagger-name</var>)</code> forms.</dd>
<dt><code>all</code></dt>
  <dd>Matches all tags and taggers. Useful if you only want to specify what you don't want, e.g. <code>(and all (not (or <var>types to exclude...</var>)))</code>. Note that <code>(not all)</code> means no tags or taggers will be selected.</dd>
<dt><code>default</code></dt>
  <dd>Shorthand for what you get if you don't specify <code>:type</code> at all (see <a href="#sec-3.1.4.">-default-type</a> above).</dd>
</dl>

<p>For example, if you wanted to get all part of speech tags except those produced by minipar, you would use <code>:type (and (type pos) (not (source minipar)))</code></p>

<p>In general, you don't need to worry about the order of taggers/tag types in the <code>:type</code> argument. Each tagger has lists of tag types that it requires as input, that it may optionally use as input, and that it may output. TextTagger uses this information to select taggers that output types required by already-selected taggers, and to sort the final list of taggers to be run so that each tagger has all the input tags it needs when it runs. There are no cycles in the <a href="#sec-5.4.">dependency graph</a> formed by all the taggers and types in TextTagger, except that some taggers output some of the same tag types that they optionally take as input. These loopy taggers are sorted after other taggers outputting that type and before other taggers inputting that type.</p>
</div>

<div class="section">
<h4>:format</h4>
<p><code>:format <var>native|lattice</var></code> (default lattice)</p>
<p>Get exactly the information TextTagger uses internally to represent the tags
(<code>native</code>), or transform that into a format the Parser likes
(<code>lattice</code>). See the section on <a href="#sec-4.">output formats</a>.</p>
</div>

<div class="section">
<h4>:imitate-keyboard-manager</h4>
<p><code>:imitate-keyboard-manager <var>boolean</var></code> (default nil)</p>
<p>Send the same kinds of messages that KeyboardManager would send, sending both the whole utterance and each word (tag) individually in its own message. Reply with a message like <code>(reply :content (ok :uttnums <var>list-of-utterance-numbers</var>))</code>, giving all the utterance numbers output for this request.</p>
<p><code>sentence</code> tags are required in order to make uttnums. If <code>:type</code> doesn't already have a source of <code>sentence</code> tags (Sentences, OneSentencePerLine, or StanfordCoreNLP), the Sentences tagger will be selected by mapping <code>:type <var>type</var></code> to <code>:type (or sentences <var>type</var>)</code>. Note that using both dedicated sentence taggers at the same time will lead to bad results. But if StanfordCoreNLP and another sentence tagger are selected, StanfordCoreNLP will consume rather than produce sentence tags, so there will be no conflict.</p>
<p>See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h4>:split-clauses</h4>
<p><code>:split-clauses <var>boolean</var></code> (default nil)</p>
<p>(implies that <code>:type</code> includes <code>clauses</code>) Treat each clause from the Clauses tagger as its own utterance. Note that while other taggers may produce clause tags, only those from the Clauses tagger are used, since it in turn uses the clauses from other taggers to filter its clause boundaries. When used without <code>:imitate-keyboard-manager t</code>, this still adds uttnums and some of the extra messages that that option would, but the messages are packed into the reply rather than sent individually. See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h4>:split-sentences</h4>
<p><code>:split-sentences <var>boolean</var></code> (default nil)</p>
<p>Treat each sentence as its own utterance. When used without <code>:imitate-keyboard-manager t</code>, this still adds uttnums and some of the extra messages that that option would, but the messages are packed into the reply rather than sent individually. See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h4>:paragraph</h4>
<p><code>:paragraph <var>boolean</var></code> (default t if :split-* is t and :imitate-keyboard-manager is t, nil otherwise)</p>
<p>Treat the text as a paragraph. That is, send <code>start-</code> and <code>end-paragraph</code> messages before and after the rest of the messages, respectively. It's useful to set e.g. <code>:split-clauses t :paragraph nil</code> if you're sending the sentences in a paragraph individually through TextTagger, and you want to send <code>start-</code> and <code>end-paragraph</code> yourself. See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h4>:next-uttnum</h4>
<p><code>:next-uttnum <var>integer</var></code> (default 1 + the last uttnum output, or 0 if :paragraph was t last time)</p>
<p>Use the given number for the next :uttnum in the output. You can set this to 0 for the first in a sequence of <code>:paragraph nil</code> requests representing a paragraph, and the following requests will use the automatically incremented value. See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h4>:fill-gaps</h4>
<p><code>:fill-gaps <var>boolean</var></code> (default t)</p>
<p>Extend each tag's endpoint to cover all the characters until the next start of a tag.</p>
</div>

<div class="section">
<h4>:sense-bracketing</h4>
<p><code>:sense-bracketing <var>tree</var></code> (default nil)</p>
<p>This argument is the source of information for the gold tagger. Leaves of
the tree have the format <code>(<var>POS</var> <var>[LF type]</var>
<var>word</var>)</code>. Internal nodes have the format
<code>(<var>category</var> <var>subtree</var> <var>[subtree ...]</var>)</code>.
For example, you might provide the following tree for the sentence "drive the
truck":
<blockquote><samp>
(S (VP (VP- (V CAUSE-TO-MOVE drive)
            (NP (SPEC (DET (ART the)))
                (N1 (N LAND-VEHICLE truck))))))
</samp></blockquote>
Then you would get the following tags (shown here in lattice format) from the gold tagger:
<blockquote><samp>
(word "drive" :frame (0 5) :sense-info ((:trips-parts-of-speech (W::V) :ont-types (ONT::CAUSE-TO-MOVE))))
(word "the" :frame (6 9) :sense-info ((:trips-parts-of-speech (W::ART))))
(prefer "the" :frame (6 9) :trips-cats (W::DET W::SPEC))
(word "truck" :frame (10 14) :sense-info ((:trips-parts-of-speech (W::N) :ont-types (ONT::LAND-VEHICLE))))
(prefer "truck" :frame (10 14) :trips-cats (W::|N1|))
(prefer "the truck" :frame (6 14) :trips-cats (W::NP))
(prefer "drive the truck" :frame (0 14) :trips-cats (W::VP- W::VP))
</samp></blockquote>
</p>
</div>

<div class="section">
<h4>:input-tags</h4>
<p><code>:input-tags <var>tags</var></code> (default nil)</p>
<p>This argument is the source of tags for the Input tagger. The tags should be in <a href="#sec-4.2.">native format</a>. They will be included as-is in TextTagger's list of tags.</p>
</div>

<div class="section">
<h4>:input-terms</h4>
<p><code>:input-terms <var>tags</var></code> (default nil)</p>
<p>If this argument is given, it's equivalent to <a href="#sec-3.6.">the load-input-terms request</a>. If it's not given, the input terms list is not affected.</p>
</div>

<div class="section">
<h4>:tell-each-tag</h4>
<p><code>:tell-each-tag <var>boolean</var></code> (default nil)</p>
<p>Instead of replying, send a <code>tell</code> message for each tag. This is
kind of like <code>:imitate-keyboard-manager t</code>, except that it doesn't
send the <code>started-speaking</code>, <code>stopped-speaking</code>, or
<code>utterance</code> messages, and it doesn't automatically add the
<code>:channel</code>, <code>:uttnum</code>, etc. arguments. When no tags would be output (e.g. when <code>:type</code> is <code>(not all)</code>), this instead sends a single <code>word</code> message with the whole text in it (the Parser then splits it into words by itself).</p>
</div>

<div class="section">
<h4>:extra-tag-args</h4>
<p><code>:extra-tag-args <var>keyword argument list</var></code> (default nil)</p>
<p>A list of extra keyword arguments to append to each tag. This was briefly
used (with <code>:tell-each-tag</code>) by SenseKeyboardManager to provide the
<code>:channel</code>, <code>:uttnum</code>, etc. arguments. Now it's only used
internally when <a href="#sec-3.1.5.">-process-input-utterances</a> is
specified. Note that if you include <code>:uttnum</code> in this list, it will set TextTagger's internal uttnum counter. Doing this is incompatible with the <code>:split-<var>*</var> t</code> options because they increment the uttnum within a single tag request.</p>
</div>

<div class="section">
<h4>:meta-map-sources</h4>
<p><code>:meta-map-sources <var>all | list of UMLS source vocabulary abbreviations</var></code> (default same as -meta-map-sources)</p>
<p>This argument overrides the command-line argument <a href="#sec-3.1.7.">-meta-map-sources</a> on a per-request basis.</p>
</div>

<div class="section">
<h4>:drum-species</h4>
<p><code>:drum-species <var>all | list of species names</var></code> (default same as -drum-species)</p>
<p>This argument overrides the command-line argument <a href="#sec-3.1.8.">-drum-species</a> on a per-request basis.</p>
</div>

<div class="section">
<h4>:use-wordfinder</h4>
<p><code>:use-wordfinder <var>boolean</var></code> (default same as -use-wordfinder)</p>
<p>This argument overrides the command-line argument <a href="#sec-3.1.16.">-use-wordfinder</a> on a per-request basis. Notably, it is used when WordFinder itself is sending a request to TextTagger, in order to avoid reentering WordFinder.</p>
</div>

<p>Note: <code>:imitate-keyboard-manager</code> and <code>:split-clauses</code>
have a strange interaction with clauses and sentences. When both are
<code>t</code>, the <code>utterance</code> messages correspond to clauses. When
<code>:imitate-keyboard-manager</code> is <code>t</code> but
<code>:split-clauses</code> is <code>nil</code>, only one
<code>utterance</code> message is sent. In both cases, <code>:uttnum</code>
corresponds to sentences (not clauses or the whole input string). Because of
this, <code>:type <var>foo</var></code> sometimes gets mapped to <code>:type (or
sentences <var>foo</var>)</code> (see
<a href="#sec-3.5.4.">:imitate-keyboard-manager</a>). When
<code>:split-clauses</code> is <code>t</code>, sentence and clause tags are not
output, but without it, they are. This includes the case
<code>:imitate-keyboard-manager t :split-clauses nil</code>, where you'll get
sentence tags even if you didn't explicitly ask for them. See also <a href="#sec-4.1.3.">splitting utterances</a>.</p>
</div>

<div class="section">
<h3>The load-, add-, get-, and clear-input-terms requests</h3>
<p>These requests manage the input terms list, which is the source of tags for the TermsInput tagger (note that these requests do <em>not</em> activate the TermsInput tagger by themselves; you must also add <code>terms-input</code> to <a href="#sec-3.5.2."><code>:type</code></a> or <a href="#sec-3.1.4."><code>-default-type</code></a>). The tags should be in <a href="#sec-4.2.">native format</a>, except they shouldn't include <code>:start</code>/<code>:end</code> offsets. The tagger searches the text for the value of the <code>:lex</code> argument (or failing that, <code>:text</code>), and tags each instance using a copy of the input tag(s) that matched. The requests are of the form <code>(request :receiver TextTagger :content <var>content</var>)</code>, with each kind of <code><var>content</var></code> described below:</p>
<dl>
<dt><code>(load-input-terms :input-terms <var>tags</var>)</code></dt>
<dd>Replace the input terms list with the given list of tags. This is equivalent to using <a href="#sec-3.5.12.">the <code>:input-terms</code> argument</a> to <a href="#sec-3.5.">the tag request</a>. Replies with <code>(reply :content (ok))</code>.</dd>
<dt><code>(add-input-terms :input-terms <var>tags</var>)</code></dt>
<dd>Add the given list of tags to those already in the input terms list. Replies with <code>(reply :content (ok))</code></dd>
<dt><code>(get-input-terms)</code></dt>
<dd>Retrieve the current contents of the input terms list. The reply will be of the form <code>(reply :content <var>tags</var>)</code>.</dd>
<dt><code>(clear-input-terms)</code></dt>
<dd>Remove all tags from the input terms list. <!-- not anymore: This also happens when TextTagger receives <code>(tell :content (start-conversation))</code>. --> TextTagger replies to the request with <code>(reply :content (ok))</code>.</dd>
</dl>
</div>

<div class="section">
<h3>Running TextTagger remotely</h3>
<p>NOTE: this is no longer the way we solve this problem. See <a href="#sec-3.1.6.">-remote-meta-map</a>.</p>

<p>In the TRIPS CER/NL system, it's sometimes necessary to run TextTagger on a
different computer from the rest of the system (a server). This was because
MetaMap (used in CER/NL) used to only run on Linux and Solaris, not Mac or
Windows. It's possible to start TextTagger on the server and give it a
<code>-connect</code> command-line argument that causes it to connect back to
the Facilitator on the local machine (instead of the default localhost, which
would be the server). This is standard TRIPS module stuff.</p>

<p>However, we also want the connection to be secure, since we might be passing
patient data through it. Using <code>-connect</code> alone doesn't do that.
Instead, the CER/NL start script (<code>trips-cernl</code>) is set up to create
a secure tunnel from the server back to the local Facilitator (when you give it
the <code>-remote-texttagger</code> option), using the <code>-R</code> option
to <code>ssh</code>. <code>ssh</code> then allocates a port to listen for
connections from the server, which TextTagger can connect to in order to get to
the local Facilitator. It allocates these ports dynamically so that multiple
clients can run instances of TextTagger and have them connect back to the right
Facilitators.</p>

<p>Unfortunately, <code>ssh</code> doesn't tell the command it's running on the
server what port it just allocated. It tells the client, but that doesn't help
when you're running it non-interactively like we are from the start script.
Instead, we run the <code>get-ssh-tunnel</code> program (installed with
TextTagger) on the server to get the right port. It does so by running the
<code>lsof</code> utility to find an open port owned by the current user whose
name is the port number prefixed by <code>localhost:</code> (which also happens
to be the right thing to pass to TextTagger's <code>-connect</code> argument).
This means that each user can only run one TextTagger instance at a time, but
that's better than only one user at a time being able to use the server.</p>

<p>Another issue is that MetaMap requires two server processes of its own.
These can be shared among different users. Normally, TextTagger will start and
stop these processes each time it is run, but this would be bad when it is run
remotely, because one user might stop the server processes when another user is
still using them. To get around this, these two processes should be started
separately from TextTagger, by a user who won't be using the server otherwise
(using the <code>wsdserverctl</code> and <code>skrmedpostctl</code> scripts
included with MetaMap). That way, TextTagger will see that the processes are
already started and not start them itself, and it won't be able to stop them
because it doesn't have permission to kill another user's processes.</p>
</div>

<div class="section">
<h3>MetaMapServer.pl</h3>
<p><code>MetaMapServer.pl</code> is the server side of <a href="#sec-3.1.6."><code>-remote-meta-map</code></a>. It accepts connections on port 6300, reads text until a blank line, runs half of the MetaMap tagger on the text, and writes the half-finished tags in native format back through the socket. It is responsible for running the MetaMap program itself, transforming its output using xsltproc, and adding more information to the tags from the UMLS database. The client side is responsible for filtering tags based on which source vocabularies they have, and mapping to ONT types.</p>

<p>Running <code>MetaMapServer.pl</code> requires certain tables from UMLS and the NCI ontology to be set up in a MySQL database. This requires about 4GB of disk space (in addition to the space taken by the MetaMap program). There are some rules in the Makefile that should help with this, but they're currently kind of messy. There is some support for using SQLite instead of MySQL, but you'll need to edit MetaMap.pm to use that.</p>
</div>

<div class="section">
<h3>text-to-lattice-test.pl</h3>
<p>In addition to the TextTagger TRIPS module, there is a more-or-less
standalone script <code>text-to-lattice-test.pl</code>. It accepts plain text
on standard input, and prints lattice-test lisp format to standard output. It
accepts all the same command-line arguments as TextTagger. It is not installed
anywhere, and it expects to be run from the <code>src/TextTagger/</code>
directory.</p> 

<p><code>text-to-lattice-test.pl</code> works by making a simple subclass of
TextTagger that redirects KQML output to <code>/dev/null</code>, and overrides
<code>imitateKeyboardManagerUtterance</code> to make it write lattice-test
forms to standard output. Then it instantiates that subclass with the
<code>-connect no</code> argument, reads each line of the input, and sends a
fake KQML message using <code>receive_request</code>. The taggers in the <a href="#sec-5.2.">Table of Taggers</a> with an X in the "requires TRIPS" column will not work in this mode, since they make requests of other TRIPS modules and would hang while waiting for a reply.</p>
</div>
</div>

<div class="section">
<h2>Output formats</h2>
<p>TextTagger supports two different formats for outputting tags: <code>lattice</code>, and <code>native</code>. The default is <code>lattice</code>, and the format can be changed using the <a href="#sec-3.5.3."><code>:format</code></a> argument to <a href="#sec-3.5.">the tag request</a>. The <code>native</code> format is much closer to how TextTagger represents tags internally, while the <code>lattice</code> format is more compact, and easier for the Parser to deal with.</p>

<div class="section">
<h3>Lattice format</h3>
<p>TextTagger's <code>lattice</code> output format is defined operationally, in terms of the effects each message has on the Parser's operation. Several tags may contribute to a single <code>lattice</code> format message. There are three major types of message: <code>word</code>, <code>prefix</code> and <code>prefer</code>. All have the form:</p>
<blockquote><code>(type "text" :frame (start end) args...)</code></blockquote>
<p>Where:</p>
<dl>
<dt><code>type</code></dt><dd>is the message type</dd>
<dt><code>"text"</code></dt><dd>is the substring of the input string the message talks about</dd>
<dt><code>start</code> and <code>end</code></dt><dd>are the start and end character offsets of that substring (but see <a href="#sec-3.5.9."><code>:fill-gaps</code></a>)</dd>
<dt><code>args...</code></dt><dd>is the rest of the list, consisting of alternating :keyword/value arguments</dd>
</dl>

<p>Each of the message types is described below, along with the arguments it may have. In addition, all types of message may have <code>:channel</code>, <code>:direction</code>, <code>:mode</code>, <code>:genre</code>, and <code>:uttnum</code> arguments, as described in <a href="#sec-3.1.5.">-process-input-utterances</a>. These arguments are single values, not lists.</p>

<div class="section">
<h4>Lattice format message types</h4>

<div class="section">
<h5>Word</h5>
<p>This message causes the Parser to build constituents based on the information in the message and add them directly to the chart. If there isn't enough information in the message itself to build constituents for a particular sense, it causes the Parser to look up the described word(s) in the lexicon (which may include using WordFinder to look it/them up in WordNet).</p>

<p>At most one <code>word</code> message will be output for each <code>:frame</code>. Different senses of the same word are represented in the <code>:sense-info</code> list. Each item of this list is a list of keyword arguments describing a set of senses. Each argument is a list of options, and selecting one option from a POS argument and one from a sense argument gives you a single sense. The following arguments may be specified in a <code>:sense-info</code> item, but are not required:</p>

<dl>
<dt><code>:penn-parts-of-speech</code></dt>
<dd><a href="http://www.cis.upenn.edu/~treebank/">Penn Treebank</a> style part of speech tags (I and II, they didn't change), from <a href="http://bulba.sdsu.edu/jeanette/thesis/PennTags.html#Word">this list</a>. Note that these actually contain morphological information as well as the basic part of speech. For example, verbs are split into <code>VB, VBD, VBG, VBN, VBP, VBZ</code>.</dd>
<dt><code>:trips-parts-of-speech</code> (deprecated)</dt>
<dd>TRIPS-style parts of speech. These are now just mapped from the <code>:penn-parts-of-speech</code>.</dd>
<dt><code>:wn-sense-keys</code></dt>
<dd><a href="http://wordnet.princeton.edu/">WordNet</a> 3.0 <a href="http://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html#sect3">sense keys</a>. Note that most of the time the last two fields, <code>:head_word:head_id</code>, are empty, so TextTagger leaves them out (including the colons).</dd>
<dt><code>:ont-types</code></dt>
<dd><a href="http://www.cs.rochester.edu/research/cisd/projects/trips/lexicon/cgi/browseontology-ajax">TRIPS Ontology</a> types. Note that <a href="http://www.cs.rochester.edu/research/cisd/projects/trips/lexicon/cgi/browseontology-ajax?search=referential-sem#referential-sem"><code>ONT::referential-sem</code></a> is removed when there are other (more specific) sense options.</dd>
<dt><code>:alternate-spellings</code></dt>
<dd>Alternate spellings of the word (e.g. "color" vs. "colour"). Also includes corrected spellings. The second member of the message will retain the original spelling.</dd>
<dt><code>:score</code></dt>
<dd>(only from the Drum tagger) A single number between 0 and 1 indicating how good this sense is compared to the others, 1 being the best score. This is the maximum of the scores of the matches in the <code>:domain-specific-info</code>.</dd>
<dt><code>:domain-specific-info</code></dt>
<dd>Structures describing other pieces of information that don't affect parsing but should be carried through to the Parser's output (all of them will be carried through, not just one). Currently this is used for <a href="http://www.nlm.nih.gov/research/umls/">UMLS</a> mappings from <a href="http://mmtx.nlm.nih.gov/">MetaMap</a>, mappings from the Drum tagger, and lexical information from Specialist. In lattice format this is an assoc list mapping domains to lists of <code>:domain-specific-info</code> structures with that domain.</dd>
</dl>

<p>In order to build constituents directly, without looking up the words, both part of speech (<code>:penn-parts-of-speech</code> or <code>:trips-parts-of-speech</code>) and sense (<code>:wn-sense-keys</code> or <code>:ont-types</code>) information must be present.</p>

<p>The <code>:sense-info</code> argument is a list because we might want to allow only certain combinations of POS, sense, and <code>:domain-specific-info</code>. For example, if we want to express the fact that the word "crashes" can either be a plural noun with the sense key <code>"crash%1:11:03"</code> or a verb in 3<sup>rd</sup> person singular present tense with the sense key <code>"crash%2:30:10"</code>, but we want to exclude the possibility of mismatching the sense and the POS, we could use two different items in the <code>:sense-info</code> list:</p>
<blockquote><pre>(word "crashes" :frame (0 7)
  :sense-info (
    (:penn-parts-of-speech (NNS) :wn-sense-keys ("crash%1:11:03"))
    (:penn-parts-of-speech (VBZ) :wn-sense-keys ("crash%2:30:10"))
    )
  )
</pre></blockquote>

</div>

<div class="section">
<h5>Prefix</h5>

<p><code>prefix</code> messages are exactly like <code>word</code> messages except that they indicate the tagged string is only a prefix, connected to the following word. This information is useful when <a href="#sec-3.5.9."><code>:fill-gaps</code></a> is <code>t</code>, since then it's not obvious from the character offsets in <code>:frame</code> whether there is whitespace between two "words". But note that you will only get <code>prefix</code> messages for tags of type <code>prefix</code>, which currently only come from the Affixes tagger. There are other situations where you can get multiple <code>word</code> messages with no whitespace between them, for example CamelCase words (which are split into tags of type <code>subword</code>), and endings like "n't" (which have tag type <code>ending</code>). See the <a href="#sec-5.2.">Table of Taggers</a>.</p>

</div>

<div class="section">
<h5>Prefer</h5>
<p>This causes the Parser to prefer certain kinds of consituents, without causing it to actually build any based on the contents of the message. These are used for things we don't have sense information for, but don't want to look up in the lexicon either (i.e. phrases, not words or multiwords). These arguments may be specified, and at least one of <code>:penn-cats</code> or <code>:trips-cats</code> is required:</p>

<dl>
<dt><code>:penn-cats</code></dt>
<dd>A list of <a href="http://www.cis.upenn.edu/~treebank/">Penn Treebank</a> II style syntactic category tags, from <a href="http://bulba.sdsu.edu/jeanette/thesis/PennTags.html#Clause">this list</a> or <a href="http://bulba.sdsu.edu/jeanette/thesis/PennTags.html#Phrase">this list</a> (all three Penn lists are on the same page).</dd>
<dt><code>:enju-cats</code></dt>
<dd>A list of Enju syntactic category tags. See the <a href="http://www.nactem.ac.ku/enju/enju-manual/enju-output-spec.html">Enju output spec</a>. The <code>xcat</code> is appended to the <code>cat</code> with an underscore, and any spaces inside it are replaced with underscores (the spec says <code>xcat</code> is a space-separated list, but I haven't seen more than one yet). The original Enju cats are included for completeness; they get mapped to Penn cats and then TRIPS cats.</dd>
<dt><code>:trips-cats</code> (deprecated)</dt>
<dd>A list of TRIPS-style syntactic categories. These are now just mapped from the <code>:penn-cats</code>.</dd>
<dt><code>:score</code></dt>
<dd>Negative log probability score coming directly from the Stanford parser. This is a single value, not a list. Note that this only comes from the separate StanfordParser tagger, not from StanfordCoreNLP.</dd>
</dl>
</div>

</div>

<div class="section">
<h4>Combining tags</h4>
<p>For lattice format only, TextTagger combines tags that are for the same span of text. These may come from different taggers, or the same tagger may output multiple different tags representing different options for interpreting the text, or representing different kinds of information. For the gory details on how this combination is done, see <a href="../Perl/TextTagger/CombineTags.pm"><code>CombineTags.pm</code></a>. But the general strategy is described here.</p>
<p>There are a few things to note before we get to combining tags for the same span, which involve combining tags for different spans:</p>
<ul>
 <li>If there are quotation tags, we use them to adjust any sentence and clause tags such that: quotations are always completely within a single sentence, any sentences within quotations are converted to clauses, and clauses are always completely within or completely outside quotations. (This part is in <a href="../Perl/TextTagger/Tags2Trips.pm"><code>Tags2Trips.pm</code></a>.)</li>
 <li>If we have a sense tag for a span with no POS information at all, it usually means it's a sense tag for a multi-word expression that was never itself POS-tagged. But the individual words might have been. So we look at POS tags for spans contained within the sense tag's span, and using some simple heuristics, we try to guess the POS for the whole span. Failing that, we guess that it's a noun of some kind and print a warning. (This part is in <a href="../Perl/TextTagger/ExtraArgs.pm"><code>ExtraArgs.pm</code></a>.)</li>
 <li>We remove phrase tags that overlap with other phrase tags without nesting. For example, if the input is "A B C", and we get phrase tags covering "A B", "B C", and "A B C", the first two will be removed. We always do this; it's not affected by the <a href="#sec-3.1.15."><code>-parsers-must-agree</code></a> option.</li>
</ul>
<p>Now we come back to combining tags for the same span. We divide the set of tags into POS (with no sense), sense (with optional POS), phrase, and other tags. We also collect the set of alternate spellings (strings not tags). We discard the other tags (which are things like clause, quotation, and spaced-chunk tags that don't fit in the word/prefix/prefer messages), and if the <a href="#sec-3.1.15."><code>-parsers-must-agree true</code></a> option was given we keep a phrase tag only if all phrase taggers (parsers) that tagged the current text at all agree on it (otherwise we keep all phrase tags). We combine sense tags for the same set of senses (and parts of speech if they have them) and take the union of their <code>:domain-specific-info</code>s. Sense tags without <code>:domain-specific-info</code> are combined even if their sense sets differ, by taking the union of the sets. Any alternate spellings are added only to the senses that matched them; any left over are used to form a new sense option (though at this point it has no actual sense information, just the spelling). As a special case, if we have exactly one sense tag from any of the input taggers (Input, TermsFromFile, TermsInput, or XMLInput), we use that and discard the rest. POS tags are more complicated.</p>
<p>Each POS tag may contain a set of multiple parts of speech. We divide the set of POS tags for a span into "intersected", "unioned", and "fallback" sets, based on which tagger they came from. We take the union of the unioned tags, and try to intersect that union with all of the intersected tags. Then we try to combine those POS tags with the sense tags that already have POS tags, again by taking the intersection with each sense's POS tags (if some senses have an empty intersection but others don't, the empty ones are eliminated). In several stages I won't go into here, we weaken this whole intersection process until we actually get some information out. Any sense tags that didn't start out with POS information get the set of POS tags we had before we intersected it with any of the other sense tags. If at the end we still don't have any POS tags, we just use the union of the fallback tags. The <code>:domain-specific-info</code> from any POS tags that have it is added to each combined POS tag if the combined tag has a (non-strict) subset of the set of parts of speech from the tag with the <code>:domain-specific-info</code>.</p>
<p>Currently, POS tags from the Specialist tagger are the only fallback tags, and POS tags from the following taggers are unioned tags (the rest are intersected): POS (stanford-pos), StanfordParser, StanfordCoreNLP, CJParser, Enju, and the input taggers listed above. POS tags for multi-word expressions that were added as described above are also unioned.</p>
</div>

<div class="section">
<h4>Splitting the message sequence into utterances</h4>
<p>When using the output of the <a href="#sec-3.5."><code>tag</code></a> request as input to the Parser, you should use <a href="#sec-3.5.4."><code>:imitate-keyboard-manager t</code></a>. This will result in a sequence of messages like this, in general:</p>
<blockquote><samp>
(tell :content (start-paragraph :id <var>paragraph-id</var>))

(tell :content (started-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-1</var>))
(tell :content (stopped-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-1</var>))
[word, prefix, and prefer messages for sentence 1, clause 1, with <var>extra-tag-args</var> and :uttnum <var>uttnum-1</var>]
(tell :content (utterance <var>extra-tag-args</var> :uttnum <var>uttnum-1</var> :text "<var>sentence-1-clause-1-text</var>"))

(tell :content (started-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-1</var>))
(tell :content (stopped-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-1</var>))
[word, prefix, and prefer messages for sentence 1, clause 2, with <var>extra-tag-args</var> and :uttnum <var>uttnum-1</var>]
(tell :content (utterance <var>extra-tag-args</var> :uttnum <var>uttnum-1</var> :text "<var>sentence-1-clause-2-text</var>"))

(tell :content (started-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-2</var>))
(tell :content (stopped-speaking <var>extra-tag-args</var> :uttnum <var>uttnum-2</var>))
[word, prefix, and prefer messages for sentence 2, clause 1, with <var>extra-tag-args</var> and :uttnum <var>uttnum-2</var>]
(tell :content (utterance <var>extra-tag-args</var> :uttnum <var>uttnum-2</var> :text "<var>sentence-2-clause-1-text</var>"))

...

(tell :content (end-paragraph :id <var>paragraph-id</var>))
(reply :content (ok :uttnums (<var>uttnum-1</var> <var>uttnum-2</var> ...)))
</samp></blockquote>
<p>There are several things to note about this, though:</p>
<ul>
 <li>A chunk of messages starting with <code>started-speaking</code> and ending with <code>utterance</code> represents an "utterance".</li>
 <li>With <a href="#sec-3.5.5."><code>:split-clauses t</code></a>, utterances correspond to clauses, but <code>:uttnum</code> counts sentences, so you can have multiple utterances with the same <code>:uttnum</code>.</li>
 <li>With only <a href="#sec-3.5.6."><code>:split-sentences t</code></a>, utterances correspond to sentences, so in this case utterances and <code>:uttnum</code>s are 1:1.</li>
 <li>With neither (the default, <code>:split-clauses nil :split-sentences nil</code>), you will only get one utterance and one <code>:uttnum</code> for the whole input text.</li>
 <li>Using <a href="#sec-3.1.5."><code>-process-input-utterances yes</code></a> and the actual KeyboardManager will have a similar effect (no splitting).</li>
 <li>The <code>UTT</code> structures that come out of the Parser mostly correspond to utterances, not <code>:uttnum</code>s (though they can be further fragmented in <code>COMPOUND-COMMUNICATION-ACT</code>s). Yes, the naming is confusing.</li>
 <li>You will only get the <code>start/end-paragraph</code> messages with <a href="#sec-3.5.7.">:paragraph t</a> (which is the default when splitting utterances and imitating KeyboardManager; otherwise it's <code>nil</code>).</li>
 <li>The <var>extra-tag-args</var> come from the <a href="#sec-3.5.14."><code>:extra-tag-args</code></a> argument (or the input <code>utterance</code> message), and will usually be something like <code>:channel Desktop :direction input</code>; specifying <code>:imitate-keyboard-manager t</code> will add those arguments if they're not already specified with <code>:extra-tag-args</code>.</li>
</ul>

<p>When utterance splitting is requested (<code>:split-clauses t</code> or <code>:split-sentences t</code>), TextTagger will ensure the appropriate tags are produced by modifying the <a href="#sec-3.5.2."><code>:type</code></a> argument if necessary. If no sentence tags would otherwise be produced (not counting those produced by the input taggers, which might produce any type of tag), the <code>sentences</code> tagger will be added to the <code>:type</code>. If clause splitting is requested, the <code>clauses</code> tagger will be added (even if other clause tags might be produced, e.g. by CoreNLP).</p>

<p>However, merely specifying a <code>:type</code> (or <a href="#sec-3.1.4."><code>-default-type</code></a>) that would produce sentence or clause tags is not sufficient to turn on sentence or clause splitting. You must also use <code>:split-clauses t</code> or <code>:split-sentences t</code> as appropriate, if you want multiple utterances as in the above sample message sequence.</p>

<p>Specifying <code>:imitate-keyboard-manager t</code> without splitting will also ensure that sentence tags are produced (in the same way as <code>:split-sentences t</code>), but they will then merely become <code>prefer</code> messages.</p>

<p>Note that there are no corresponding <a href="#sec-3.1.">command-line</a> (or <a href="#sec-3.1.1.">config-file</a>) parameters for <code>:split-clauses</code> and <code>:split-sentences</code>, and you can't set them with <a href="#sec-3.2."><code>set-parameters</code></a>. You can only use them directly with a <code>tag</code> request. This is because they affect not just which tags are output or what is in each tag, but the overall structure of the sequence of output tags/messages. Whatever TRIPS module is sending input to TextTagger (and presumably expecting corresponding output from the Parser or IM) needs to know about that structure.</p>

<p>If you're using the Lisp functions defined in <code><var>$TRIPS_BASE</var>/src/Systems/core/test.lisp</code> to send input through TextTagger to the Parser, you can select how you want utterances to be split by setting the Lisp variable <code>*texttagger-split-mode*</code> to <code>:split-clauses</code> or <code>:split-sentences</code>. Setting it to <code>nil</code> disables utterance splitting. The <code>*use-texttagger*</code> Lisp variable must also be set to <code>t</code> for these functions to send input via TextTagger at all (if it's <code>nil</code>, the input goes directly to the Parser instead). This <em>only</em> applies to input sent using these functions, not input sent via other modules such as KeyboardManager, SpeechIn, WebParser (including <code>batch.rb</code>), or DrumGUI, nor input sent using the <code>p</code> function defined in the Parser itself.</p>

</div>

</div>

<div class="section">
<h3>Native format</h3>
<p>TextTagger's <code>native</code> output format is defined denotationally, in terms of what each tag and each tag argument says about the input string. Tags are output in the form <code>(type args...)</code>, where <code>type</code> is the tag type (i.e. one of the tag types in the <a href="#sec-5.3.">table of tag types</a>, not the message type from <a href="#sec-4.1"><code>lattice</code> format</a>), and <code>args...</code> is the rest of the list, consisting of alternating :keyword/value arguments. The following arguments are used (many of these are the same as in the previous section):</p>

<dl>
<dt><code>:lex</code></dt>
<dd>The lexical item tagged.</dd>
<dt><code>:text</code></dt>
<dd>The part of the text tagged. This is basically the same as <code>:lex</code>, except that it's used for tags that are decidedly un-word-like, like sentences and quotations.</dd>
<dt><code>:start</code> and <code>:end</code></dt>
<dd>The character indices of the start and end of the substring tagged.</dd>
<dt><code>:penn-pos</code></dt>
<dd>Penn-style part-of-speech tags. If you're giving <code>native</code> format tags to one of the input taggers (e.g. via the <a href="#sec-3.6.">load-input-terms request</a>), please note that specifying <code>:penn-pos</code> on a <code>sense</code> tag is not sufficient to make all the parts of speech appear in the combined <code>lattice</code> format output (see <a href="#sec-4.1.2.">Combining tags</a> for details). If you want this you should also include a separate tag of type <code>pos</code>, e.g.
<blockquote><samp>
(sense :lex "IL-8" :lftype (ONT::PROTEIN) :penn-pos (NN NNP))
(pos :lex "IL-8" :penn-pos (NN NNP))
</samp></blockquote>
</dd>
<dt><code>:pos</code></dt>
<dd>TRIPS-style part-of-speech tags. (deprecated)</dd>
<dt><code>:penn-cat</code></dt>
<dd>Penn-style syntactic categories.</dd>
<dt><code>:enju-cat</code></dt>
<dd>Enju-style syntactic categories.</dd>
<dt><code>:cat</code></dt>
<dd>TRIPS-style syntactic categories. (deprecated)</dd>
<dt><code>:lftype</code></dt>
<dd>TRIPS ontology types. This was named before we switched from the LF to the ONT package for these symbols.</dd>
<dt><code>:wn-sense-keys</code></dt>
<dd>WordNet 3.0 sense keys.</dd>
<dt><code>:domain-specific-info</code></dt>
<dd>UMLS/Drum/Specialist-specific structure. This argument has a slightly different format from the same argument in <a href="#sec-4.1."><code>lattice</code> format</a>: it has only one DSI structure instead of an assoc list of them, and the domain to be used as the key in the assoc list should go in the <code>:domain</code> argument instead. If you're giving <code>native</code> format tags to one of the input taggers (e.g. via the <a href="#sec-3.6.">load-input-terms request</a>), and you want to see <code>:domain-specific-info ((<var>domain</var> (<var>type</var> <var>args...</var>)))</code> in the <code>lattice</code> format output, you should write the <code>native</code> format argument as <code>:domain-specific-info (<var>type</var> :domain <var>domain</var> <var>args...</var>)</code> instead. If you want more than one DSI structure in the <code>lattice</code> format output, you need to put them in whole separate tags that are <a href="#sec-4.1.2.">combinable</a>. For example, <code>native</code> format
<blockquote><samp>
(sense :lftype (ont::molecule) :lex "GDP" :penn-pos (NNP)
  :domain-specific-info (term :domain drum :id PC::8977))
(sense :lftype (ont::molecule) :lex "GDP" :penn-pos (NNP)
  :domain-specific-info (term :domain drum :id CHEBI::17552))
(sense :lftype (ont::molecule) :lex "GDP" :penn-pos (NNP)
  :domain-specific-info (term :domain something-else :id SE::a-made-up-id-123))
</samp></blockquote>
becomes <code>lattice</code> format
<blockquote><samp>
(word "GDP" :frame (<var>start</var> <var>end</var>) :sense-info ((
  :penn-parts-of-speech (NNP) :trips-parts-of-speech (W::NAME)
  :ont-types (ONT::MOLECULE)
  :domain-specific-info (
    (something-else
      (term :id SE::a-made-up-id-123))
    (drum
      (term :id PC::8977)
      (term :id CHEBI::17552))
    )
  )))
</samp></blockquote>
</dd>
<dt><code>:score</code></dt>
<dd>When used in phrase tags from StanfordParser: the negative log probability of this phrase. When used in sense tags from drum: the maximum match score for any of the matches used to find this sense (not any kind of probability, just a number between 0 and 1, 1 being the best match).</dd>
<dt><code>:source</code></dt>
<dd>The name of the tagger this tag came from.</dd>
</dl>

</div>

</div>

<div class="section">
<h2>Tagger reference</h2>
<p>This section describes all the individual taggers that make up TextTagger.</p>

<div class="section">
<h3>A word on naming</h3>
<p>The names of the various tags are used in different forms for different
purposes. Suppose you wanted to add a new type of tag called "mome rath":
<ul>
<li>The tagger would go in <code>Perl/TextTagger/MomeRaths.pm</code>.</li>
<li>The function to call in that file would be called <code>tag_mome_raths</code>.</li>
<li>The <code>type</code> key in the tags produced by that function would get the value <code>mome-rath</code>.</li>
<li>The name of the tagger used in the <code>:type</code> argument would be <code>mome-raths</code> or <code>mome_raths</code> (they're equivalent).</li>
</ul>
</p>

<p>There are a few exceptions to this:
<ul>
<li>Some taggers produce tags with different names (e.g. Terms produces <code>named-entity</code> tags).</li>
<li>The tagger in POS.pm is named <code>stanford_pos</code>.</li>
</ul>
(sorry about this, it's partly a result of naming conventions clashing across
different languages)</p>
</div>

<div class="section">
<h3>Table of taggers</h3>
<table>
<tr><th>Tagger</th><th>Tag type(s)</th><th class="rotate"><div><span>initializable</span></div></th><th class="rotate"><div><span>requires TRIPS</span></div></th><th>Description</th></tr>
<tr>
<td>Affixes</td>
<td>prefix, word</td>
<td></td>
<td>X</td>
<td>Splits common affixes (just prefixes for now) from words.</td>
</tr><tr>
<td>Alphanumerics</td>
<td>named-entity</td>
<td></td>
<td></td>
<td>Marks sequences of capital letters, numbers, and hyphens as named
entities with lftype <code>REFERENTIAL-SEM</code> (e.g. "AK-47").</td>
</tr><tr>
<td>AlternateSpellings</td>
<td>alternate-spelling</td>
<td>X</td>
<td></td>
<td>Looks up alternate spellings for words in a simple tab-separated values (tsv) file (currently fetched and converted from <a href="https://wiki.ubuntu.com/EnglishTranslation/WordSubstitution">this page</a>).</td>
</tr><tr>
<td>Asma</td>
<td>alternate-spelling, sense</td>
<td>X</td>
<td></td>
<td>Expands texting acronyms and tags asthma drug names for the TRIPS Asma system.</td>
</tr><tr>
<td>CapitalizedNames</td>
<td>named-entity</td>
<td></td>
<td></td>
<td>Marks any capitalized word not at the beginning of a sentence as a
named-entity with LF type <code>REFERENTIAL-SEM</code>.</td>
</tr><tr>
<td>ChemicalFormulae</td>
<td>sense</td>
<td></td>
<td></td>
<td>Marks chemical formulae (with element abbreviations, counts, bonds, groups, etc.) with LF type <code>CHEMICAL</code> and Penn POS <code>NNP</code>.</td>
</tr><tr>
<td>CJParser</td>
<td>clause, phrase, pos</td>
<td>X</td>
<td></td>
<td>Uses the Charniak-Johnson Parser (with McClosky's biomed parsing model) to parse sentences, and converts the resulting parse trees to clause, phrase, and POS tags.</td>
</tr><tr>
<td>Clauses</td>
<td>clause</td>
<td></td>
<td></td>
<td>Splits sentences into clauses at certain punctuation (e.g. commas), as long
as the clauses end up long enough, and they agree with other sources of clause tags if enabled. Only clauses from this tagger are used for <a href="#sec-3.5.5."><code>:split-clauses t</code></a> mode. See also <a href="#sec-4.1.3.">splitting utterances</a>.</td>
</tr><tr>
<td>Countries</td>
<td>named-entity</td>
<td>X</td>
<td></td>
<td>Looks up country names and related terms from <a href="https://github.com/mledoze/countries">mledoze/countries</a>' <code>countries.json</code> file.</td>
</tr><tr>
<td>Disambiguator</td>
<td>sense</td>
<td></td>
<td>X</td>
<td>Calls the Disambiguator TRIPS module to run a statistical classifier and assign scores to previously found ambiguous sense tags.</td>
</tr><tr>
<td>Drum</td>
<td>sense</td>
<td>X</td>
<td>X</td>
<td>Looks up biological mechanism terms for the Deep Reader for Understanding Mechanisms (DRUM). See the section on <a href="#sec-5.7.">The Drum tagger</a>.</td>
</tr><tr>
<td>Enju</td>
<td>clause, phrase, pos</td>
<td>X</td>
<td></td>
<td>Uses the Enju parser (with the GENIA model) to parse sentences, and converts the resulting parses to clause, phrase, and POS tags.</td>
</tr><tr>
<td>Gold</td>
<td>phrase, pos, sense, word</td>
<td></td>
<td></td>
<td>Returns tags according to the given <code>:sense-bracketing</code> argument. This was used in the STEP system to provide the parser with perfect ("gold standard") information in these areas.</td>
</tr><tr>
<td>Input</td>
<td>(anything)</td>
<td></td>
<td></td>
<td>Accepts <a href="#sec-4.2.">native tags</a> in the <a href="#sec-3.5.11."><code>:input-tags</code></a> argument and integrates them into TextTagger's output as if they originated within TextTagger. It's possible some tags won't end up being output due to conflicts with other tags (particularly phrase tags).</td>
</tr><tr>
<td>MetaMap</td>
<td>sense</td>
<td>X</td>
<td></td>
<td>Uses UMLS MetaMap to find senses of words and phrases in the medical domain.</td>
</tr><tr>
<td>Minipar</td>
<td>phrase, pos</td>
<td></td>
<td>X</td>
<td>Uses Minipar (via the Minipar TRIPS module) to parse the string.
Gets phrase tags for internal nodes of the parse tree, and pos tags for the
leaves. Note that the Minipar TRIPS module pretty much only works on the URCS
network, since it references an installation of Minipar in someone's home
directory there. Also, it doesn't work with CMUCL due to a problem it has
finding a PTY to run Minipar on. Also note that you can't use this tagger with
<a href="#sec-3.9."><code>text-to-lattice-test.pl</code></a>, since it requires
a real KQML connection to talk to the Minipar TRIPS module.</td>
</tr><tr>
<td>Misspellings</td>
<td>alternate-spelling, subword</td>
<td>X</td>
<td></td>
<td>Uses <a href="http://aspell.net/">GNU Aspell</a> to correct misspellings. Sometimes aspell will split words; in that case this tagger also outputs subword tags. See <a href="#sec-3.1.18.">-aspell-dict</a>.</td>
</tr><tr>
<td>Music</td>
<td>sense</td>
<td></td>
<td></td>
<td>Tags music-related terms and abbreviations, such as pitches, intervals, and chords.</td>
</tr><tr>
<td>NamedEntities</td>
<td>named-entity</td>
<td>X</td>
<td></td>
<td>Uses the Stanford NER to find named entities with lftypes
<code>ORGANIZATION</code>, <code>PERSON</code>, or
<code>GEOGRAPHIC-REGION</code>.</td>
</tr><tr>
<td>NamesFromFile</td>
<td>named-entity</td>
<td>X</td>
<td></td>
<td>Tags names listed in the file specified with the <code>-names-file</code> option (one per line)</td>
</tr><tr>
<td>OneSentencePerLine</td>
<td>sentence</td>
<td></td>
<td></td>
<td>Tags nonempty lines as sentences. See also <a href="#sec-4.1.3.">splitting utterances</a>.</td>
</tr><tr>
<td>PersonalNames</td>
<td>named-entity</td>
<td>X</td>
<td></td>
<td>Uses the <a href="https://www.ssa.gov/oact/babynames/limits.html">SSA names list</a> to tag personal names, with their gender if it's not too ambiguous.</td>
</tr><tr>
<td>PlaceNames</td>
<td>named-entity</td>
<td>X</td>
<td>X</td>
<td>Looks up place names from several resources (including those of the Countries and Terms taggers, among others), in a similar way to the Drum tagger.</td>
</tr><tr>
<td>POS (stanford-pos)</td>
<td>pos</td>
<td>X</td>
<td></td>
<td>Uses the Stanford POS tagger to find (Penn) parts of speech for the words
in the string.</td>
</tr><tr>
<td>Prescriptions</td>
<td>pos</td>
<td></td>
<td></td>
<td>Tag certain (often abbreviated) phrases occurring in prescriptions as either adjectives or adverbs</td>
</tr><tr>
<td>PseudoMetaMap</td>
<td>sense</td>
<td>X</td>
<td></td>
<td>Tag a few specific word senses that we've found MetaMap fails to tag. This is a complement to, not a replacement for, MetaMap</td>
</tr><tr>
<td>Punctuation</td>
<td>punctuation</td>
<td></td>
<td></td>
<td>Each punctuation character gets its own tag.</td>
</tr><tr>
<td>Quotations</td>
<td>quotation</td>
<td></td>
<td></td>
<td>Tag double-quoted strings. These quotation tags aren't directly output in <a href="#sec-4.1.">lattice format</a>, but instead <a href="#sec-4.1.2.">affect sentence and clause tags</a>.</td>
</tr><tr>
<td>RomanNumerals</td>
<td>number</td>
<td></td>
<td></td>
<td>Tag roman numerals as numbers, including the case and numeric value in <code>:domain-specific-info</code>.</td>
</tr><tr>
<td>Sentences</td>
<td>sentence</td>
<td></td>
<td></td>
<td>Uses the <a href="http://search.cpan.org/~shlomoy/Lingua-EN-Sentence-0.25/lib/Lingua/EN/Sentence.pm"><code>Lingua::EN::Sentence</code></a>
Perl module to split the string into sentences. See also <a href="#sec-4.1.3.">splitting utterances</a>.</td>
</tr><tr>
<td>SpacedChunks</td>
<td>spaced-chunk</td>
<td></td>
<td></td>
<td>Splits the string on sequences of whitespace longer than one character.
Used in the PLOT system to help identify columns of data in a character cell
grid.</td>
</tr><tr>
<td>Specialist</td>
<td>pos</td>
<td>X</td>
<td></td>
<td>Adds information from the SPECIALIST lexicon, including part of speech, citation form, complement patterns, and derivational links.</td>
</tr><tr>
<td>StanfordCoreNLP</td>
<td>clause, named-entity, phrase, pos, punctuation, sentence, word</td>
<td>X</td>
<td></td>
<td>Uses Stanford CoreNLP to process the string (or each sentence tag if they're available), using the tokenizer to produce <code>word</code> and <code>punctuation</code> tags, the sentence splitter to produce <code>sentence</code> tags (unless they were already in the input), the POS tagger to produce <code>pos</code> tags, the parser to produce <code>clause</code> and <code>phrase</code> tags, and the NER to produce <code>named-entity</code> tags.</td>
</tr><tr>
<td>StanfordParser</td>
<td>clause, phrase, pos</td>
<td>X</td>
<td></td>
<td>Uses the Stanford Parser to parse sentences, and converts the resulting
parse trees to clause, phrase, and POS tags.</td>
</tr><tr>
<td>StreetAddresses</td>
<td>street-address</td>
<td></td>
<td></td>
<td>Uses the <a href="http://search.cpan.org/~sderle/Geo-StreetAddress-US-0.99/US.pm"><code>Geo::StreetAddress::US</code></a>
Perl module to tag anything that could be the first line of a US address (e.g. "40 S. Alcaniz St.").</td>
</tr><tr>
<td>Terms</td>
<td>named-entity</td>
<td>X</td>
<td></td>
<td>Tags any substring that matches a name in the GNIS database as a
named-entity with LF type <code>GEOGRAPHIC-REGION</code>.</td>
</tr><tr>
<td>TermsFromFile</td>
<td>(anything)</td>
<td>X</td>
<td></td>
<td>Tags anything matching the first column of the <a href="#sec-3.1.12."><code>-terms-file</code></a> with all the tags in the rest of the columns on that row. Matching is done case-insensitively, and allows the insertion of various kinds of dash.</td>
</tr><tr>
<td>TermsInput</td>
<td>(anything)</td>
<td></td>
<td></td>
<td>Tags anything matching the <a href="#sec-4.2."><code>:lex</code> (or <code>:text</code>) argument</a> of any of the tags from the input terms list with a copy of the matching tag(s). See <a href="#sec-3.6.">the load-, add-, get-, and clear-input-terms requests</a> and <a href="#sec-3.5.12.">the <code>:input-terms</code> argument</a> to <a href="#sec-3.5.">the tag request</a>. Matching is done case-insensitively, and allows the insertion of various kinds of dash.</td>
</tr><tr>
<td>Units</td>
<td>sense</td>
<td>X</td>
<td></td>
<td>Tags units of measure as nouns with lftypes under <code>MEASURE-UNIT</code> according to their dimensions. Uses the installed <code>units</code> program (e.g. <a href="https://www.gnu.org/software/units/">GNU Units</a>) for its unit definitions. The <code>:units</code> fields of the <code>:domain-specific-info</code> structures produced by the Units tagger may be fed directly to the UnitConverter2 TRIPS module's <code>convert-units</code> request.</td>
</tr><tr>
<td>VariantLists</td>
<td>alternate-spelling</td>
<td></td>
<td></td>
<td>Tags non-first variants in slash- or ampersand-separated lists of variants like "Smad1/5/8" as alternate spellings that include the prefix, like "5" -&gt; "Smad5" and "8" -&gt; "Smad8". The Words tagger should already get the first one, "Smad1".</td>
</tr><tr>
<td>WordNet</td>
<td>sense</td>
<td>X</td>
<td></td>
<td>Tags multiword "words" from WordNet with their sense key, part of speech, and morphology.</td>
</tr><tr>
<td>Words</td>
<td>word, ending, number, subword, subnumber</td>
<td></td>
<td></td>
<td>Tags anything that the Parser might want to consider a separate
word. Example:
<blockquote><samp>
foo BarBaz don't 123,456 12EA56
( ) (    )(()  )         (    ) word
             ( )                ending
                 ( ) ( )        number
    ( )( )                 ()   subword
                         ()  () subnumber
</samp></blockquote>
</td>
</tr><tr>
<td>XMLInput</td>
<td>(anything)</td>
<td>X</td>
<td></td>
<td>Gets XML tags from the original text (before <a href="#sec-3.1.13."><code>-xml-tags</code></a> removes them) and maps them to TextTagger tags using the rules specified in <a href="#sec-3.1.14."><code>-xml-input-rules-file</code></a>.</td>
</tr>
</table>
</div>

<div class="section">
<h3>Table of tag types</h3>
<p>This is simply a reverse-lookup table for the above table, except that the input taggers (Input, TermsFromFile, TermsInput, and XMLInput) are omitted.</p>
<table>
<tr><th>Tag type</th><th>Tagger(s)</th></tr>
<tr><td>alternate-spelling</td><td>AlternateSpellings, Asma, Misspellings, VariantLists</td></tr>
<tr><td>clause</td><td>CJParser, Clauses, Enju, StanfordCoreNLP, StanfordParser</td></tr>
<tr><td>ending</td><td>Words</td></tr>
<tr><td>named-entity</td>
    <td>Alphanumerics, CapitalizedNames, Countries, NamedEntities, NamesFromFile, PersonalNames, StanfordCoreNLP, Terms</td></tr>
<tr><td>number</td><td>RomanNumerals, Words</td></tr>
<tr><td>phrase</td>
    <td>CJParser, Enju, Minipar, StanfordCoreNLP, StanfordParser, Gold</td></tr>
<tr><td>pos</td>
    <td>CJParser, Enju, Gold, Minipar, StanfordCoreNLP, StanfordParser, POS (stanford-pos), Prescriptions, Specialist</td></tr>
<tr><td>punctuation</td><td>Punctuation, StanfordCoreNLP</td></tr>
<tr><td>quotation</td><td>Quotations</td></tr>
<tr><td>sentence</td><td>OneSentencePerLine, Sentences, StanfordCoreNLP</td></tr>
<tr><td>sense</td><td>ChemicalFormulae, Disambiguator, Drum, Gold, MetaMap, Music, Units, WordNet</td></tr>
<tr><td>spaced-chunk</td><td>SpacedChunks</td></tr>
<tr><td>street-address</td><td>StreetAddresses</td></tr>
<tr><td>subnumber</td><td>Words</td></tr>
<tr><td>subword</td><td>Affixes, Misspellings, Words</td></tr>
<tr><td>word</td><td>Gold, StanfordCoreNLP, Words</td></tr>
</table>
</div>

<div class="section">
<h3>Tagger/tag type dependency graph</h3>
<p>This automatically-generated bipartite graph shows how taggers and tag types in TextTagger depend on each other. You can think of data as flowing along the edges forwards. And, roughly speaking, you can think of specifying a <a href="#sec-3.5.2."><code>:type</code></a> as selecting a subset of the nodes in this graph. Then TextTagger completes that subgraph by tracing the connected black edges backwards all the way to "input text", and forwards to the next tag type node(s) if a selected node was a tagger node. There is no final output node in the graph, because all of the tag types can affect the output (also, there are some non-tagger output stages not pictured; see <a href="#sec-4.1.2.">Combining tags</a>).</p>
<p>Also note that each TRIPS system only includes a subset of the taggers. If your system includes Graphviz, you can regenerate this graph for that subset by running <code>make clean &amp;&amp; make</code> in <code><var>$TRIPS_BASE</var>/src/TextTagger/docs/</code>. Please take care not to commit such a partial graph. In order to generate the full graph you must have all of the taggers checked out.</p>
<a href="tagger-deps.svg"><img src="tagger-deps.svg" style="max-width: 100%" /></a>
</div>

<div class="section">
<h3>Table of LF types</h3>
<p>Some tags (especially named-entity tags) have an associated LF type, and
different LF types are emitted by different taggers. Note that sense tags
emitted by the gold tagger (not listed in this table) may have any LF type,
since the LF types are provided in the <code>:sense-bracketing</code> argument
to the tag request. Sense tags emitted by the Disambiguator tagger may also have any LF type, as long as it was emitted by another tagger.</p>
<table>
<tr><th>LF type</th><th>Tagger(s)</th></tr>
<tr><td>ALLERGY-MEDICATION</td><td>Asma</td></tr>
<tr><td>CHEMICAL</td><td>ChemicalFormulae</td></tr>
<tr><td>CHORD</td><td>Music</td></tr>
<tr><td>CITY</td><td>Countries</td></tr>
<tr><td>COUNTRY</td><td>Countries</td></tr>
<tr><td>FEMALE-PERSON</td><td>PersonalNames</td></tr>
<tr><td>GEOGRAPHIC-REGION</td><td>Countries, NamedEntities, StanfordCoreNLP, Terms</td></tr>
<tr><td>LOCATION-ID</td><td>StreetAddresses</td></tr>
<tr><td>LONG-TERM-CONTROL-DRUG</td><td>Asma</td></tr>
<tr><td>MALE-PERSON</td><td>PersonalNames</td></tr>
<tr><td>MEASURE-UNIT</td><td>Units (also uses subtypes)</td></tr>
<tr><td>NATIONALITY</td><td>Countries</td></tr>
<tr><td>NATIONALITY-VAL</td><td>Countries</td></tr>
<tr><td>ORGANIZATION</td><td>NamedEntities, StanfordCoreNLP</td></tr>
<tr><td>PERSON</td><td>NamedEntities, PersonalNames, StanfordCoreNLP</td></tr>
<tr><td>PITCH</td><td>Music</td></tr>
<tr><td>PITCH-INTERVAL</td><td>Music</td></tr>
<tr><td>QUICK-RELIEF-DRUG</td><td>Asma</td></tr>
<tr><td>REFERENTIAL-SEM</td><td>Alphanumerics, CapitalizedNames, NamesFromFile, StanfordCoreNLP</td></tr>
<tr><td>many different types</td><td>Drum, MetaMap</td></tr>
</table>
</div>

<div class="section">
<h3>Table of POS/cats</h3>
<p>Similarly, some types of tags have fixed default parts of speech or syntactic categories:</p>
<table>
<tr><th>POS/cat argument</th><th>types of tags</th></tr>
<tr><td>:penn-parts-of-speech (NNP NNPS)</td><td>named-entity tags</td></tr>
<tr><td>:penn-parts-of-speech (NNP)</td><td>street-address tags, sense tags from ChemicalFormulae, and sense tags from MetaMap whose lftype is <a href="http://www.cs.rochester.edu/research/cisd/projects/trips/lexicon/cgi/browseontology-ajax?search=geographic-region#geographic-region"><code>ONT::geographic-region</code></a></td></tr>
<tr><td>:penn-parts-of-speech (NN NNP)</td><td>sense tags from MetaMap whose lftype is <a href="http://www.cs.rochester.edu/research/cisd/projects/trips/lexicon/cgi/browseontology-ajax?search=substance#substance"><code>ONT::substance</code></a></td></tr>
<tr><td>:penn-cats (S SBARQ SINV SQ)</td><td>sentence tags</td></tr>
</table>
</div>

<div class="section">
<h3>The Drum tagger</h3>
<p>The Drum tagger looks up biological mechanism terms for the Deep Reader for Understanding Mechanisms (DRUM). It's more complicated than most of the other taggers included as part of TextTagger, so it deserves its own section.</p>

<div class="section">
<h4>Building</h4>
<p>At build time, we fetch a number of data sources (see the table at the end of this section) and use them to build one big table mapping words and phrases ("terms") to the relevant information from those data sources (<code>drum-terms.tsv</code>). That information includes IDs (sometimes called "accession numbers") from those data sources, information about the status of the term as associated with each ID, and the preferred term for the ID. For some sources (right now just UniProt) we also get database cross-references ("dbxrefs") relating the IDs across different sources, and put them in a secondary table (<code>drum-dbxrefs.tsv</code>). We also convert some data sources to DeepSemLex Lisp format, in order to use their hierarchy information.</p>

<p>The terms are normalized (by <code>Normalize.pm</code>), but the pre-normalized form of each term is also stored in the table. Normalization converts upper case letters to lower case, removes dashes (including several different dash characters defined in Unicode, as well as the humble ASCII hyphen-minus character), and collapses each string of whitespace to a single ASCII space character. For some data sources we do some extra processing before normalization. Some sources always capitalize their terms, even if that's not how they would appear in ordinary text, so we attempt to undo that. And some sources include greek letters spelled out, like "alpha" instead of "α", so we include both the spelled-out version and a version with the greek letter "unspelled". The table is sorted lexicographically by normalized term, to make searching it faster.</p>
</div>

<div class="section">
<h4>Lookup</h4>
<p>At run time, the Drum tagger takes input from a number of different other taggers, in order to get word and sentence segmentation and part of speech options, as well as alternative spellings<!-- including VariantLists --> for some words. It converts most of these input tags to simple lattice entries, containing only start and end character offsets and an unnormalized word. The citation form of each word that was tagged by the Specialist tagger is also added to the lattice. This is mostly to get the singular form of plurals ("depluralizing"), but it also includes words that were already singular with minor spelling differences (e.g. input "lipooxygenase" vs. citation form "lipoxygenase"). As a special case, if an input tag changes a word that's all-caps except for a final "s" to all-lower-case, we change it back in the lattice. For example the input "GAPs" might get a tag that spells it "gaps", and we'd change it back to "GAPs". This affects scoring later on.</p>

<p>This lattice is then matched against the normalized terms from the table (by <code>terms2.cpp</code>). We recursively construct each possible term from contiguous lattice entries, subject to length limits in number of characters (100), and number of lattice entries (20). Each constructed term is normalized, and then looked up in the table.<!-- We don't normalize each lattice entry first, because some of the things normalization removes would come between entries, so we have to normalize the constructed term anyway. --> The table is large enough that we don't want to keep all of it in memory, so we keep it on disk, with a small in-memory index associating every 100th term with its byte offset in the table file. We look up the term in the index to find which 100-term block it would be in, and then read from the start of that block to the search term or the end of the block, whichever comes first. We also use an in-memory Bloom filter to avoid using the disk (or the index) at all most of the time, in the common case where a constructed term is not in the table.</p>

<p>We immediately discard a few kinds of bad matches. If the term has a space in it, but one or both ends are inside a word (which can happen due to some of the sub-word splitting that the Words tagger does), we discard that match. For example, we discard a match for "T cell" in "HEK293T cell". And if one or both ends of a term are dashes inside a word, for example "-Ras" in "GEF-Ras", we discard that match. And if the term is entirely composed of prefixes and dashes, joined to the following word without a dash (which can happen when the Affixes tagger is active), we discard that match. For example, we discard the match for (gene) "re-re" in "re-reactivation".</p>
</div>

<div class="section">
<h4>Scoring</h4>
<p>The result of the table lookup has both the unnormalized form of the term constructed from the lattice entries, and the unnormalized form of each matching term from one of our data sources. These two forms are not in general the same, but they do normalize to the same string. We count the ways in which they differ (which can only involve letter case and the presence or absence of dashes; we ignore the specific type of dash used, and the amount of whitespace), and that contributes to a match score, along with the status of the term in the data source it came from, and whether a depluralized or spelling-corrected lattice entry contributed to the match. Higher scores are better (i.e. they are not costs, but benefits).</p>

<p>The status score is a simple hand-built ranking of the status annotations from all of the data sources, ranging from 0 to 5 (except that we add 1 for terms from the FamPlex (FPLX) source, so it's really 0 to 6):</p>
<table>
<tr><th>score</th><th>meaning</th><th>status annotations</th></tr>
<tr><td>5</td><td>the most official name or abbreviation</td>
<td><code>NM</code>, <code>RecName: <var>*</var></code>, <code>name</code>, <code>Approved<var>*</var></code>, <code>I*</code>, <code>ID</code> (except XFAM)</td></tr>
<tr><td>4</td><td>an alternative name for the exact same thing</td>
<td><code>ID</code> (only XFAM), <code>synonym</code>, <code>EXACT synonym</code>, <code>RELATED synonym</code> (only BTO and CVCL)</td></tr>
<tr><td>3</td><td>something that could be a synonym or related term</td>
<td><code>RELATED synonym</code> (except BTO and CVCL), <code>SY</code>, <code>DE</code>, <code>WK</code>, <code>AltName: <var>*</var></code>, <code>Synonym</code></td></tr>
<tr><td>2</td><td>definitely an inexact synonym</td>
<td><code>NARROW synonym</code>, <code>BROAD synonym</code></td></tr>
<tr><td>1</td><td>a previous name or abbreviation</td>
<td><code>PI</code>, <code>Previous<var>*</var></code></td></tr>
<tr><td>0</td><td>a fake entry added by us</td>
<td><code>fake</code></td></tr>
</table>
<p>(The <code>fake</code> status is for the <code>UP:etc</code> IDs added to the table to replace the excessive number of IDs that certain terms in UniProt have.)</p>

<p>The spelling correction score is 0 if any spelling correction was necessary to match the term, 1 otherwise.</p>

<p>The depluralization score is basically 2 if no depluralization was necessary to match the term, 1 if only one word in the term was depluralized, and 0 if more than one word was depluralized. But we also add the worst (i.e. minimum) score of any depluralized word in the term, which is between 0 and 1. That comes from the Specialist tagger, and it uses only the case/dash match score, because the other two components are not available to it. In total, the depluralization score is still between 0 and 2, because when the base score is 2 there is no worst score for a depluralized word, because there is no depluralized word.</p>

<blockquote><pre>depluralization_score =
    _
   |  2 <b>if</b> 0 lattice entries were depluralized
  &lt;   1 + (the score of the depluralized entry) <b>if</b> 1 lattice entry was depluralized
   |_ (the minimum score of all depluralized entries) <b>otherwise</b>
</pre></blockquote>

<p>The case/dash match score has several components, which are mostly based on counting the number of places in which the two unnormalized forms of the term differ in particular ways. So first we need to characterize those differences. Call the form derived from the input text A, and the form derived from the data source B. We align A and B so that the corresponding letters line up, and split them into the same sequence of words, such that if there is a dash in either A or B, it will split the word it is in in both A and B. We also count the dashes in A that aren't in B (<code>dash-no-dash</code>) and vice versa (<code>no-dash-dash</code>). (As a special case, we discard matches that involve such dash mismatches between digits.) Then we check whether each word matches exactly in A and B (<code>exact</code>), and if they don't, we characterize the case patterns of A's version and B's version and count that pairing of characterizations. The possible case pattern characterizations are:</p>
<dl>
<dt><code>sentence-cap</code></dt><dd>The word begins a sentence and is a single capital letter followed by zero or more lower-case letters (this only happens for A, because only the input text has sentences).</dd>
<dt><code>single-cap</code></dt><dd>The whole word is a single capital letter (except <code>sentence-cap</code>).</dd>
<dt><code>all-caps</code></dt><dd>All capital letters (except <code>single-cap</code> and <code>sentence-cap</code>).</dd>
<dt><code>no-caps</code></dt><dd>No capital letters.</dd>
<dt><code>initial-cap</code></dt><dd>The word is a single capital letter followed by one or more lower-case letters (except <code>sentence-cap</code>).</dd>
<dt><code>mixed-caps</code></dt><dd>Anything else.</dd>
</dl>

<p>For example, if the match is at the beginning of a sentence, A is "Foo-BarbAz gLaRcH", and B is "FOObar-bAz GlArCh", then both A and B would be split into the words "foo bar baz glarch", and the following counts would result:</p>
<ul>
 <li><code>:dash-no-dash 1</code> (between foo and bar)</li>
 <li><code>:no-dash-dash 1</code> (between bar and baz)</li>
 <li><code>:sentence-cap-all-caps 1</code> (beginning-of-sentence Foo vs. FOO)</li>
 <li><code>:initial-cap-no-caps 1</code> (Bar vs. bar)</li>
 <li><code>:exact 1</code> (bAz)</li>
 <li><code>:mixed-caps-mixed-caps 1</code> (GlArCh vs. gLaRcH)</li>
</ul>
<p>Note that the non-dash-related counts add up to the total number of words in the term (4). Also note that <code>mixed-caps-mixed-caps</code> does not count as an exact match because caps can be mixed in different ways.</p>

<p>Once we have characterized the match between A and B, we compute the case/dash score based on that characterization. This score has several subcomponents. Ranked from most to least important, they are:</p>
<dl>
 <dt><code>short_abbr</code> (2 possible values)</dt>
  <dd>If B is 3 characters or fewer, and an <code>all-caps</code> word matches a <code>no-caps</code> word in either direction, this score is 0, otherwise it is 1. This is meant to penalize matches between short abbreviations (usually <code>all-caps</code>) and short regular words, e.g. "for" vs. "FOR" (HGNC:12799).</dd>
 <dt><code>mixed</code> (3)</dt>
  <dd>This score penalizes mixed-caps mismatches, which are usually obviously wrong. If there are any <code>mixed-caps-mixed-caps</code> matches, this score is 0; otherwise, if there are any non-exact word matches that involve <code>mixed-caps</code> at all, this score is 1; otherwise this score is 2.</dd>
 <dt><code>exact</code> (2)</dt>
  <dd>This score rewards exact matches. If there are any <code>exact</code> matches, this score is 1, otherwise it is 0.</dd>
 <dt><code>acic</code> (3)</dt>
  <dd>If there are only <code>exact</code> or <code>sentence-cap-no-caps</code> matches, this score is 2; otherwise, if the only non-exact word matches are <code>all-caps-initial-caps</code> (which is particularly common), this score is 1; otherwise this score is 0.</dd>
 <dt><code>combo</code> (5)</dt>
  <dd>This score penalizes case mismatches in general, and penalizes having a combination of many different kinds of mismatches. It is computed separately for A and B and then the better (larger) of the two scores is used. It starts at 4, but one point is deducted for each different word case characterization that participates in an inexact match. For this, <code>single-cap</code> is not treated as different from <code>all-caps</code> or <code>initial-cap</code> (but the latter two are still different from each other). Similarly, <code>sentence-cap</code> is not treated as different from <code>single-cap</code>, <code>initial-cap</code>, <code>no-caps</code>. In addition, if the only case mismatch is <code>sentence-cap-no-caps</code>, it doesn't count, and this score is 4. All these exceptions just ensure that matches aren't penalized for things that can't be helped, like the length of words, and whether A is at the beginning of the sentence.</dd>
 <dt><code>dash</code> (3)</dt>
  <dd>This score is 2 if there are no dash mismatches, 1 if there is only one kind of dash mismatch, and 0 if there are both <code>dash-no-dash</code> and <code>no-dash-dash</code> mismatches. Dash mismatches are extremely common, so this is the least important component of the score.</dd>
</dl>

<p>These subcomponents are combined as if they were digits in a number. The base depends on which place the digit is in, since the subcomponents are integers with different ranges. This ensures that the smallest possible change in a more-important subcomponent has a larger effect on the case/dash score than the largest possible change in a less-important subcomponent. The number is then divided by the product of the "bases", minus one. This ensures that the case/dash score is between 0 and 1. The final formula (formatted to emphasize the digits-in-a-number interpretation) is:</p>
<blockquote><pre>case_dash_score =
  (((((((0
  ) * 2 + short_abbr
  ) * 3 + mixed
  ) * 2 + exact
  ) * 3 + acic
  ) * 5 + combo
  ) * 3 + dash
  ) / 539
</pre></blockquote>

<p>Finally, the status score, spelling correction score, depluralization score, and case/dash score are themselves combined in a similar way:</p>
<blockquote><pre>final_score =
  (((((0
  ) * 7 + status_score
  ) * 2 + spelling_correction_score
  ) * 3 + depluralization_score
  ) * 2 + case_dash_score
  ) / 83
</pre></blockquote>
</div>

<div class="section">
<h4>Ontology mapping</h4>

<p>Most of our data sources have is-a hierarchies, or something similar. When we look up a term and get an ID belonging to one of these data sources, we travel up the hierarchy until we reach an ID with a mapping into the TRIPS ontology. These mappings are defined in <code>drum-mappings.lisp</code>. Traversal of the hierarchy, as well as caching, are handled separately from TextTagger by the DeepSemLex TRIPS module. Some data sources use multiple inheritance, so this can result in multiple mappings to different ONT types. If a traversal yields no TRIPS mappings for a match, the match is discarded.</p>

<p>For other data sources (and certain parts of those with hierarchies), we have a few ad hoc mapping rules depending on which data source the ID is from:</p>

<p>IDs from GO where the name from GO ends in " activity" but the input text lacks that ending are mapped to <code>ONT::protein</code>. At build time the variant without the ending is added to the table for such terms under the molecular_function concept in GO. We noticed that most of these terms described the activity of some protein, and some of those proteins we didn't get from other sources. If the input <em>does</em> have the " activity" ending, we still tag the part before it as <code>ONT::protein</code>, but put the <code>:domain-specific-info</code> for the whole term only on the word "activity", and map the ID to TRIPS normally.</p>

<p>We skip IDs from CHEBI and CVCL for matches that depend on depluralization.</p>

<p>IDs from HGNC all map to <code>ONT::gene</code>.</p>

<p>IDs from UniProt's subcell terms map to different ONT types depending on the entry type (which is included after the ID in the table). Location entries (<code>ID</code>) map to <code>ONT::cell-part</code>. Topology entries (<code>IT</code>) map to <code>ONT::protein-family</code>. And orientation entries (<code>IO</code>) map to <code>ONT::cell-part</code>.</p>

<p>Other UniProt IDs map to <code>ONT::protein</code>.</p>

<p>PFam IDs map to different ONT types depending on the <code>TP</code> field. "Family" maps to <code>ONT::protein-family</code>, while everything else (Domain, Repeat, Motif) maps to <code>ONT::molecular-domain</code>.</p>

<p>IDs from MeSH&reg; SCR map to <code>ONT::pharmacologic-substance</code>.</p>

<p>IDs from CVCL map to <code>ONT::cell-line</code>.</p>

<p>IDs from neXtProt-family (FA) map to <code>ONT::protein-family</code>.</p>

<p>We get IDs from HP via EFO, when they are linked to MeSH&reg; and/or DOID. These map to <code>ONT::medical-disorders-and-conditions</code>.</p>
</div>

<div class="section">
<h4>Amino acids, protein sites, and mutations</h4>

<p>In addition to the main table lookup and ontology mapping mechanisms, the Drum tagger has a couple of custom specialized sub-taggers. One of them tags amino acids, protein sites, and protein-level mutations, based in large part on this <a href="http://www.hgmd.cf.ac.uk/docs/mut_nom.html">Nomenclature for the description of sequence variations</a>.</p>

<p>Amino acids map to <code>ONT::amino-acid</code>. Sometimes the normal table lookup mechanism also tags an amino acid, but that tag is discarded in favor of the tag from this mechanism. Amino acid tags have <code>:domain-specific-info</code> of type <code>amino-acid</code>, containing the full <code>:name</code> of the amino acid as well as the single <code>:letter</code> abbreviation.</p>

<p>Protein sites map to <code>ONT::molecular-site</code>. Their tags have <code>:domain-specific-info</code> of type <code>aa-site</code>, which include the same fields as those of amino acids, as well as an <code>:index</code> specifying the position of the amino acid in the chain of amino acids that makes up the protein.</p>

<p>Mutations map to <code>ONT::mutation</code> Their tags have <code>:domain-specific-info</code> of type <code>mutation</code>, but they also have a subordinate <code>:type</code> field, which can be one of <code>insertion</code>, <code>deletion</code>, or <code>substitution</code>. Their other fields depend on which sub-type it is and how it was written. They can include:</p>
<dl>
<dt><code>:old</code></dt><dd>The amino acid that was deleted (an <code>amino-acid</code>-type <code>:domain-specific-info</code> structure).</dd>
<dt><code>:new</code></dt><dd>The amino acid(s) that were inserted (can be an <code>amino-acid</code> DSI or a list thereof).</dd>
<dt><code>:aa-index</code></dt><dd>The amino-acid index where the mutation took place (similar to <code>:index</code> from <code>aa-site</code>)</dd>
<dt><code>:lower</code></dt><dd>The <code>aa-site</code> DSI for the start of the mutation (the side with the lower index).</dd>
<dt><code>:upper</code></dt><dd>Ditto for the end/upper index.</dd>
</dl>

</div>

<div class="section">
<h4>miRNAs</h4>

<p>The other specialized sub-tagger tags micro RNAs or miRNAs, based on the description of the standard nomenclature for them on <a href="http://en.wikipedia.org/wiki/MicroRNA#Nomenclature">Wikipedia</a>. In short, a miRNA name consists of an optional 3-4 letter species abbreviation, followed by the letters "mir", followed by an ID number in a specific format, which we match but don't do anything further with. The parts may have dashes between them. The case pattern of the "mir" part signifies different possible referents: "miR" refers to the mature form and maps to <code>ONT::molecule</code>; "mir" refers to a precursor or primary form and also maps to <code>ONT::molecule</code>; and "MIR" refers to the gene that encodes them, and maps to <code>ONT::gene</code>. The <code>:domain-specific-info</code> for miRNA tags is of type <code>mirna</code>, and has these fields:</p>
<dl>
<dt><code>:type</code></dt><dd>The sub-type of miRNA, one of <code>mature</code>, <code>precursor-or-primary</code>, or <code>gene</code>.</dd>
<dt><code>:number</code></dt><dd>The number part of the miRNA name, the part that comes after "mir".</dd>
<dt><code>:species</code> (optional)</dt><dd>The species part of the miRNA name, which comes before "mir", expanded to the full genus+species Latin name. The mapping from species abbreviations to full names is in <code>mirna-species.tsv</code>.</dd>
</dl>
</div>

<div class="section">
<h4>Discarding more tags</h4>

<p>At this point we discard any tags with IDs from UniProt for the wrong species, if the <a href="#sec-3.1.8.">-drum-species</a> or <a href="#sec-3.5.16.">:drum-species</a> arguments were given.</p>

<p>We also discard tags for all-lower-case single words that are already in the TRIPS lexicon (including prefixes).</p>
</div>

<div class="section">
<h4>POS tagging</h4>

<p>We add one or more of the four noun Penn POS tags (<code>NN</code>, <code>NNP</code>, <code>NNS</code>, <code>NNPS</code>) to each tag output by the Drum tagger. We treat each of the four total values of the two features encoded in these POS tags separately: singular, plural (<code>S</code>), common, and proper (<code>P</code>).</p>

<p>If a tag resulted from one of the specialized taggers built into the Drum tagger rather than the generic table-lookup mechanism, we say it is singular and not plural. Otherwise, if it could have matched using a depluralized lattice entry, we say it can be plural; if it could have matched using no such entries, we say it can be singular. Note that the last two options are not mutually exclusive.</p>

<p>If the input text for a tag contains a capital letter or a digit, we say it can be proper. If it doesn't, or if it consists only of a capital letter followed by lower case letters, spaces, and/or dashes, then we say it can be common. Note that these are not mutually exclusive.</p>
</div>

<div class="section">
<h4>Table of data sources</h4>
<table>
<tr><th>data source name/link</th><th>ID prefix</th><th>file format</th><th>maps to TRIPS types</th><th>description</th></tr>
<tr>
 <td><a href="http://www.brenda-enzymes.info/">BRENDA</a> Tissue Ontology</td>
 <td>BTO</td>
 <td><a href="http://owlcollab.github.io/oboformat/doc/GO.format.obo-1_2.html">OBO</a></td>
 <td>CELL, CELL-LINE, INTERNAL-BODY-PART</td>
 <td>"A structured controlled vocabulary for the source of an enzyme. It comprises terms of tissues, cell lines, cell types and cell cultures from uni- and multicellular organisms."</td>
</tr>
<tr>
 <td><a href="https://code.google.com/p/cell-ontology/">Cell Ontology</a> (basic)</td>
 <td>CL</td>
 <td>OBO</td>
 <td>CELL</td>
 <td>"The Cell Ontology is designed as a structured controlled vocabulary for cell types. This ontology was constructed for use by the model organism and other bioinformatics databases, where there is a need for a controlled vocabulary of cell types. This ontology is not organism specific; indeed it includes cell types from prokaryotes to mammals, including plants and fungi."</td>
</tr>
<tr>
 <td><a href="http://www.ebi.ac.uk/chebi">Chemical Entities of Biological Interest (ChEBI)</a></td>
 <td>CHEBI</td>
 <td>OBO</td>
 <td>CHEMICAL, MOLECULE</td>
 <td>"A freely available dictionary of molecular entities focused on ‘small’ chemical compounds."</td>
</tr>
<tr>
 <td><a href="http://www.nextprot.org/">neXtProt</a> Cellosaurus</td>
 <td>CVCL</td>
 <td>OBO</td>
 <td>CELL-LINE</td>
 <td>"a controlled vocabulary of cell lines"</td>
</tr>
<tr>
 <td>databases</td>
 <td>DB</td>
 <td>TSV</td>
 <td>DATABASE</td>
 <td>This is our own list of databases that might be mentioned in the bio domain.</td>
</tr>
<tr>
 <td><a href="http://www.ebi.ac.uk/efo/">Experimental Factor Ontology</a></td>
 <td>EFO</td>
 <td>OBO</td>
 <td>CANCER, CELL-LINE, MEASURE-UNIT</td>
 <td>"The Experimental Factor Ontology (EFO) provides a systematic description of many experimental variables available in EBI databases, and for external projects such as the NHGRI GWAS catalogue. It combines parts of several biological ontologies, such as anatomy, disease and chemical compounds. The scope of EFO is to support the annotation, analysis and visualization of data handled by many groups at the EBI and as the core ontology for the Centre for Therapeutic Validation (CTTV). Functional Genomics Team. We also add terms for external users when requested." For now we are using only the cell line and unit pieces of this ontology, which include parts of BTO and UO with some extensions native to EFO.</td>
</tr>
<tr>
 <td><a href="http://www.nextprot.org/">neXtProt</a> controlled vocabulary of families</td>
 <td>FA</td>
 <td>EMBL?</td>
 <td>PROTEIN-FAMILY</td>
 <td>Protein families from neXtProt's <code>cv_family.txt</code>. "Developed in collaboration between the SIB Swiss Institute of Bioinformatics and Geneva Bioinformatics (GeneBio) SA, neXtProt will be a comprehensive human-centric discovery platform, offering its users a seamless integration of and navigation through protein-related data."</td>
</tr>
<tr>
 <td><a href="https://github.com/sorgerlab/famplex">FamPlex</a> (formerly Bioentities)</td>
 <td>FPLX</td>
 <td>OBO</td>
 <td>PROTEIN-FAMILY (possibly more in the future)</td>
 <td>"FamPlex is a collection of resources for grounding biological entities from text and describing their hierarchical relationships. Resources were developed by manual curation for use by natural language processing and biological modeling teams in the DARPA Big Mechanism and Communicating with Computers programs."</td>
</tr>
<tr>
 <td><a href="http://www.genenames.org/">HUGO Gene Nomenclature Committee</a></td>
 <td>HGNC</td>
 <td><a href="http://www.genenames.org/cgi-bin/statistics">TSV</a></td>
 <td>GENE</td>
 <td>"HGNC is responsible for approving unique symbols and names for human loci, including protein coding genes, ncRNA genes and pseudogenes, to allow unambiguous scientific communication." We are using just the protein coding genes for now.</td>
</tr>
<tr>
 <td><a href="http://www.geneontology.org/">Gene Ontology</a></td>
 <td>GO</td>
 <td>OBO</td>
 <td>CELL, CELL-PART, EVENT-OF-CHANGE, INTERNAL-BODY-PART, MACROMOLECULAR-COMPLEX, MOLECULE, SIGNALING-PATHWAY, TIME-SPAN</td>
 <td>"The GO project has developed three structured, controlled vocabularies (ontologies) that describe gene products in terms of their associated biological processes, cellular components and molecular functions in a species-independent manner."</td>
</tr>
<tr>
 <td><a href="http://www.nlm.nih.gov/mesh/">Medical Subject Headings (MeSH&reg;)</a> <a href="http://www.nlm.nih.gov/bsd/indexing/training/CATD_040.html">Supplementary Concept Records (SCR)</a></td>
 <td>MESH</td>
 <td><a href="http://www.nlm.nih.gov/mesh/ctype.html">ASCII MeSH&reg; SCR</a></td>
 <td>PHARMACOLOGIC-SUBSTANCE</td>
 <td>Mostly drugs and chemicals. We only get terms in the semantic types "Pharmacologic Substance" (T121), "Antibiotic" (T195), and "Clinical Drug" (T200).</td>
</tr>
<tr>
 <td><a href="http://psidev.info/">Proteomics Standards Initiative</a> for Molecular Interaction (PSI-MI)</td>
 <td>MI</td>
 <td>OBO</td>
 <td>BIOLOGICAL-ROLE, CHEMICAL, EVENT-OF-CHANGE, FORMAL-UNIT, GENE, MACROMOLECULAR-COMPLEX, MOLECULE, ORDERED-DOMAIN, PROTEIN</td>
 <td>Molecular interactions, and the interacting molecules that interact in them. (this is currently unused)</td>
</tr>
<tr>
 <td><a href="https://ncit.nci.nih.gov/ncitbrowser/">National Cancer Institute thesaurus (NCIt)</a></td>
 <td>NCIT</td>
 <td><a href="http://evs.nci.nih.gov/ftp1/NCI_Thesaurus/ReadMe.txt">FLAT (TSV)</a></td>
 <td>BIOLOGICAL-PROCESS, CANCER, CELL-LINE, CHEMICAL, FOOD, GENE, MACROMOLECULAR-COMPLEX, MANUFACTURED-OBJECT, MEDICAL-DISORDERS-AND-CONDITIONS, MRNA, ORGANISM (and many descendants), PHARMACOLOGIC-SUBSTANCE, PROCEDURE, PROTEIN, PROTEIN-FAMILY, RNA, SIGNALING-PATHWAY, SUBSTANCE</td>
 <td>"A controlled vocabulary in support of NCI administrative and scientific activities."</td>
</tr>
<tr>
 <td><a href="http://lincs.hms.harvard.edu/db/sm/">HMS LINCS Small Molecules database</a></td>
 <td>PC</td>
 <td>CSV</td>
 <td>PHARMACOLOGIC-SUBSTANCE</td>
 <td>Drugs and chemicals. We use the PubChem CIDs instead of the native HMS LINCS IDs, which is why the ID prefix is "PC".</td>
</tr>
<tr>
 <td><a href="http://pir.georgetown.edu/pro/">Protein Ontology</a></td>
 <td>PR</td>
 <td>OBO</td>
 <td>PROTEIN</td>
 <td>"PRO provides an ontological representation of protein-related entities by explicitly defining them and showing the relationships between them. Each PRO term represents a distinct class of entities (including specific modified forms, orthologous isoforms, and protein complexes) ranging from the taxon-neutral to the taxon-specific (e.g. the entity representing all protein products of the human SMAD2 gene is described in PR:Q15796; one particular human SMAD2 protein form, phosphorylated on the last two serines of a conserved C-terminal SSxS motif is defined by PR:000025934)." For now, we only use human proteins whose IDs have the PR: prefix, and are not copied from UniProt.</td>
</tr>
<tr>
 <td><a href="https://code.google.com/p/unit-ontology/">Units of Measurement Ontology</a></td>
 <td>UO</td>
 <td>OBO</td>
 <td>MEASURE-UNIT</td>
 <td>Ontology of measurement units used in biology.</td>
</tr>
<tr>
 <td><a href="http://www.uniprot.org/">UniProt</a> Swiss-Prot</td>
 <td>UP</td>
 <td><a href="http://web.expasy.org/docs/userman.html">EMBL (sorta)</a></td>
 <td>CELL-PART, PROTEIN</td>
 <td>"The UniProt Knowledgebase (UniProtKB) is the central hub for the collection of functional information on proteins, with accurate, consistent and rich annotation." Uniprot has two sections, Swiss-Prot and TrEMBL. Swiss-Prot is the hand-curated section, so it's the one we use. We also use their subcellular location vocabulary, <code>subcell.txt</code>.</td>
</tr>
<tr>
 <td><a href="http://xfam.org/">Xfam</a>/<a href="http://pfam.xfam.org/">Pfam</a>-A</td>
 <td>XFAM</td>
 <td><a href="http://en.wikipedia.org/wiki/Stockholm_format">Stockholm</a></td>
 <td>PROTEIN-FAMILY</td>
 <td>"The Pfam database is a large collection of protein families, each represented by multiple sequence alignments and hidden Markov models (HMMs). ... There are two components to Pfam: Pfam-A and Pfam-B. Pfam-A entries are high quality, manually curated families."</td>
</tr>
</table>
</div>

</div>

<div class="section">
<h3>The PlaceNames tagger</h3>
<p>The PlaceNames tagger looks up names of places, and is similar to the Drum tagger described above, so it also deserves its own section.</p>

<div class="section">
<h4>Building</h4>
<p>At build time, we fetch a number of data sources (see the table at the end of this section) and use them to build one big table mapping place names to the relevant information from those data sources (<code>place-names.tsv</code>). That information includes IDs and general categories for the places, as well as information about the status of the specific names for the places (e.g. whether it's the official name or an alternative name). The names are normalized in the same way as the Drum terms are (see <a href="#sec-5.7.1.">the corresponding section about the Drum tagger</a>).</p>

<p>Unlike <code>drum-terms.tsv</code>, the non-name information in <code>place-names.tsv</code> is already packaged as a <code>:domain-specific-info</code> structure, one of the following types.</p>

<p>For everything except <code>countries.json</code>:</p>
<blockquote><code><pre>(place
  :id <var>id</var>
  :class <var>feature-class</var>
  :source "<var>filename</var> [ row <var>row-number</var> ]"
  [ :status <var>status</var> ]
  [ :code <var>feature-code</var> ]
  )
</pre></code></blockquote>

<p>For <code>countries.json</code> (same as the Countries tagger):</p>
<blockquote><code><pre>(country
  :code <var>id</var>
  :name "<var>official-name</var>"
  :status "<var>status</var>"
  )

(capital :country <var>country-id</var> :status name)

({ region | subregion | demonym }
  :countries (<var>country-id-1</var> <var>country-id-2</var> ...)
  :status name
  )
</pre></code></blockquote>

<p>The country IDs used are ISO 3166-1 alpha-2 codes.</p>

<p>At run time, the PlaceNames tagger adds a <code>:matches</code> field similar to Drum's. It moves the <code>:source</code> and <code>:status</code> fields into the matches, and collapses DSIs that are otherwise identical, since the same input can match the same place ID via different sources. This also simplifies the scoring mechanism.</p>

<p>A subset of data sources to be used can be specified using the <code>$PLACE_NAME_SOURCES</code> environment variable at build time. For example, if you only wanted to use GNO, GADM, and mledoze/countries, you would run:</p>
<blockquote><pre>PLACE_NAME_SOURCES="GNO GADM countries" make &amp;&amp; make install</pre></blockquote>
</div>

<div class="section">
<h4>Lookup</h4>
<p>At run time, the PlaceNames tagger mainly uses the original text as input (unlike the Drum tagger). It can also use sentence tags to affect the scoring and filtering of potential names at the beginning of sentences. Otherwise place name lookup is done similarly to <a href="#sec-5.7.2.">Drum term lookup</a>.</p>
</div>

<div class="section">
<h4>Filtering</h4>
<p>The PlaceNames tagger does some of the same filtering of bad matches that <a href="#sec-5.7.2.">the Drum tagger does</a>, but it also has its own additional filters. It skips:</p>
<ul>
 <li>Matches from a stoplist that includes things like closed-class temporal names (e.g. month names), cardinal direction names (e.g. "Northeast"), and some important classes of English words that could appear capitalized at the beginning of a sentence or in title case, such as determiners and quantifiers, pronouns, question words, and auxiliary verbs.</li>
 <li>Matches starting or ending with a dash, e.g. "- The".</li>
 <li>Single words in the TRIPS lexicon (without WordNet) that are either short and at the beginning of a sentence, or uncapitalized.</li>
 <li>Matches of the form "the &lt;word&gt;", where &lt;word&gt; is in the TRIPS lexicon, and "the" is either lowercase or at the beginning of a sentence (e.g. "The River" → New Brunswick, NJ).</li>
 <li>Short names that don't match exactly (different case or dashes).</li>
 <li>Matches from GNIS that involve capitalizing a word, or are a single capitalized word at the beginning of a sentence.</li>
</ul>
</div>

<div class="section">
<h4>Scoring</h4>
<p>The PlaceNames tagger scores matches using a tiered scoring formula similar to <a href="#sec-5.7.3.">the one used by the Drum tagger</a>, and in fact uses the same <code>case_dash_score</code>. It also has a <code>status_score</code> with the following levels:</p>
<table>
<tr><th>score</th><th>meaning</th><th>status annotations</th></tr>
<tr><td>3</td><td>official/approved name</td><td><code>name</code>, <code>N</code>, <code>NS</code>, <code>official</code></td></tr>
<tr><td>2</td><td>conventional name or encoding variant</td><td><code>ascii</code>, <code>ref</code>, <code>C</code> <code>VA</code>, <code>common</code></td></tr>
<tr><td>1</td><td>alternative name</td><td><code>alternate</code>, <code>alt</code>, <code>alt1</code>, <code>alt2</code>, <code>V</code>, <code>VS</code></td></tr>
<tr><td>0</td><td>unverified name or unknown status</td><td><code>D</code>, <code>P</code>, <code>DS</code>, <code>undef</code></td></tr>
</table>
<p>And it has a <code>source_score</code> with the following levels:</p>
<table>
<tr><th>score</th><th>meaning</th><th>data source files</th></tr>
<tr><td rowspan="5">3</td><td rowspan="5">specific to Ethiopia</td>
     <td><code>gadm_woredas.txt</code> (GADM)</td></tr>
 <tr><td><code>eth_pop_adm3.csv</code>, <code>eth_populatedplaces_tabulardata.csv</code> (HDX)</td></tr>
 <tr><td><code>ET.txt</code> (GNO)</td></tr>
 <tr><td><code>et.txt</code> (GNS)</td></tr>
 <tr><td><code>eth_bnd_adm2_wfpco.csv</code>, <code>Ethiopia_bnd_adm2_woreda.csv</code> (WFP)</td></tr>
<tr><td rowspan="4">2</td><td rowspan="4">specific to South Sudan</td>
     <td><code>ssd_populatedplaces_tabulardata.csv</code> (HDX)</td></tr>
 <tr><td><code>SS.txt</code> (GNO)</td></tr>
 <tr><td><code>od.txt</code> (GNS)</td></tr>
 <tr><td><code>ssd_ica_mainsettlements_geonode_feb2016.csv</code>, <code>ssd_ica_predlhz_geonode_feb2016.csv</code> (WFP)</td></tr>
<tr><td rowspan="2">1</td><td rowspan="2">unspecific (whole world)</td>
     <td><code>countries.json</code> (mledoze/countries)</td></tr>
 <tr><td><code>admin1CodesASCII.txt</code>, <code>admin2Codes.txt</code>, <code>cities50000.txt</code> (GNO)</td></tr>
<tr><td>0</td><td>specific to USA</td><td><code>NationalFile.txt</code> (GNIS)</td></tr>
</table>
<p>The final scoring formula is:</p>
<blockquote><pre>final_score =
  ((((0
  ) * 4 + source_score
  ) * 4 + status_score
  ) * 2 + case_dash_score
  ) / 31
</pre></blockquote>
</div>

<div class="section">
<h4>Ontology mapping</h4>
<p>The ontology mappings for the PlaceNames tagger are mostly defined in <code>cwms-mappings.lisp</code>, which is loaded for the DeepSemLex TRIPS module to use. This is analogous to <a href="#sec-5.7.4."><code>drum-mappings.lisp</code> for the Drum tagger</a>, except that the GNO and GNIS hierarchies (such as they are) are also defined here, rather than in separate files under <code>cwms-dsl/</code>. And instead of mapping from the individual place IDs, we start from their categories.</p>

<p>The GNO, GADM, and GNS data sources use a common <a href="https://www.geonames.org/export/codes.html">two-level type hierarchy</a>, and while building <code>place-names.tsv</code> we also assign these codes to information we get from HDX and WFP. We use these codes to map place names from all five of those sources. GNIS has its own single level of types (feature classes) which we map separately. <code>countries.json</code> just uses a hash table defined in <code>PlaceNames.pm</code>, <code>%countries_dsi_type2ont_type</code>, which maps each <code>:domain-specific-info</code> type to a TRIPS type (the DSI type is in turn derived from which field in <code>countries.json</code> we got the name from).</p>
</div>

<div class="section">
<h4>Table of data sources and files</h4>
<table>
<tr><th>data source name/links</th><th>ID prefix</th><th>file</th><th>maps to TRIPS types</th><th>description</th></tr>
<tr><td><a href="https://github.com/mledoze/countries">mledoze/countries</a></td><td>(none)</td><td><code>countries.json</code></td><td>CITY, COUNTRY, GEOGRAPHIC-REGION, NATIONALITY, NATIONALITY-VAL</td><td>"This repository contains a list of world countries, as defined by ISO Standard 3166-1" (This is the same data used by the Countries tagger.)</td></tr>
<tr><td>
 <a href="http://www.arizona.edu/">University of Arizona</a><br/>
 <a href="http://clulab.cs.arizona.edu/">Computational Language Understanding (CLU) Lab</a><br/>
 supplementary file for GNO derived from<br/>
 <a href="https://gadm.org/">the Database of Global Administrative Areas (GADM)</a>
</td><td>GADM</td><td><code>gadm_woredas.txt</code></td><td>DISTRICT</td><td>"Eidos has provided a supplementary text file that contains woredas from GADM that are not found in the [GNO] allcountries.zip"<br/>"GADM, the Database of Global Administrative Areas, is a high-resolution database of country administrative areas, with a goal of 'all countries, at all levels, at any time period.'"</td></tr>
<tr><td>
 <a href="https://www.usgs.gov/">United States Geological Survey (USGS)</a><br/>
 <a href="https://www.usgs.gov/core-science-systems/ngp/board-on-geographic-names">Board on Geographic Names (BGN)</a><br/>
 <a href="https://www.usgs.gov/core-science-systems/ngp/board-on-geographic-names/domestic-names">Geographic Names Information System (GNIS)</a>
</td><td>GNIS</td><td><code>NationalFile.txt</code></td><td>various, mostly under GEOGRAPHIC-REGION and GEO-FORMATION</td><td>"The GNIS contains information about physical and cultural geographic features of all types in the United States, associated areas, and Antarctica, current and historical, but not including roads and highways." (This is the same data used by the Terms tagger.)</td></tr>
<tr>
 <td rowspan="5"><a href="https://www.geonames.org/">GeoNames.org</a></td>
 <td rowspan="5">GNO</td>
 <td></td>
 <td></td>
 <td>"The GeoNames geographical database covers all countries and contains over eleven million placenames that are available for download free of charge."</td>
</tr>
 <tr><td><code>admin1CodesASCII.txt</code></td><td>STATE</td><td>"names in English for admin divisions."</td></tr>
 <tr><td><code>admin2Codes.txt</code></td><td>COUNTY</td><td>"names for administrative subdivision 'admin2 code'"</td></tr>
 <tr><td><code>cities50000.txt</code></td><td>CITY, DISTRICT</td><td>all cities with a population &gt; 50,000 (derived from <code>cities15000.txt</code>)</td></tr>
 <tr><td><code>ET.txt</code>, <code>SS.txt</code></td><td>various, mostly under GEOGRAPHIC-REGION and GEO-FORMATION</td><td>"features for country with iso code XX" (ET=Ethiopia, SS=South Sudan)</td></tr>
<tr><td>
 <a href="https://www.nga.mil/">National Geospatial-intelligence Agency (NGA)</a><br/>
 <a href="http://geonames.nga.mil/gns/html/index.html">GEONet Names Server (GNS)</a>
</td><td>GNS</td><td><code>et.txt</code>, <code>od.txt</code></td><td>various, mostly under GEOGRAPHIC-REGION and GEO-FORMATION</td><td>"The GEOnet Names Server (GNS) is the official repository of standard spellings of all foreign geographic names, sanctioned by the United States Board on Geographic Names (US BGN)." (et=Ethiopia, od=South Sudan)</td></tr>
<tr>
 <td rowspan="3"><a href="https://data.humdata.org/">Humanitarian Data Exchange (HDX)</a></td>
 <td rowspan="3">HDX</td>
 <td><code>eth_pop_adm3.csv</code></td><td>DISTRICT</td>
 <td rowspan="3">"Find, share and use humanitarian data all in one place" (eth=Ethiopia, ssd=South Sudan)</td>
</tr>
 <tr><td><code>eth_populatedplaces_tabulardata.csv</code></td><td>CITY</td></tr>
 <tr><td><code>ssd_populatedplaces_tabulardata.csv</code></td><td>CITY</td></tr>
<tr>
 <td rowspan="4"><a href="https://www.wfp.org/">World Food Programme (WFP)</a> <a href="https://geonode.wfp.org/">GeoNode</a></td>
 <td rowspan="4">WFP</td>
 <td><code>eth_bnd_adm2_wfpco.csv</code></td><td>COUNTY</td>
 <td rowspan="4">(eth=Ethiopia, ssd=South Sudan)</td>
</tr>
 <tr><td><code>Ethiopia_bnd_adm2_woreda.csv</code></td><td>DISTRICT</td></tr>
 <tr><td><code>ssd_ica_mainsettlements_geonode_feb2016.csv</code></td><td>CITY</td></tr>
 <tr><td><code>ssd_ica_predlhz_geonode_feb2016.csv</code></td><td>COUNTY</td></tr>
</table>
</div>

</div>

<div class="section">
<h3>How to write and/or integrate your own tagger in TextTagger</h3>
<p>First, consider whether you actually need to do this. Look at the <a href="#sec-5.2.">table of taggers</a> to see if one of those taggers already does the thing you want to do, or can be configured to do so. The Input, NamesFromFile, TermsFromFile, TermsInput, and XMLInput taggers are particularly configurable. Many of those make use of TextTagger's <a href="#sec-4.2.">native tag format</a> for their configuration input.</p>
<p>You may instead wish to bypass TextTagger entirely, and provide your input directly to the Parser. In that case, you should familiarize yourself with the <a href="#sec-4.1.">lattice message format</a> that the Parser expects. It may also help to look at some examples of TextTagger's output with <a href="#sec-3.5.4."><code>:imitate-keyboard-manager t</code></a> passed to <a href="#sec-3.5.">the tag request</a>.</p>
<p>To write a tagger as part of TextTagger, you must write a Perl module under the <code>TextTagger</code> package (i.e. in <code><var>$TRIPS_BASE</var>/src/TextTagger/Perl/TextTagger/</code>), and it must push a hashref describing itself onto the <code>@TextTagger::taggers</code> array. You must also add a <code>use</code> statement using your module in the <code>Config.pm.in</code> file, under <code># all optional taggers</code> (preferably in alphabetical order). Note the <code>.in</code> ending: the build system uses this file as input when creating the <code>Config.pm</code> Perl module. Modify the <code>.in</code> version, not the automatically generated version. Do not commit <code>Config.pm</code>.</p>
<p>See <a href="#sec-5.1.">a word on naming</a> for guidance on how to format the name of your tagger in various contexts, in order to be consistent with the other taggers. However, you probably don't need to create your own tag <em>type</em>; you can (and probably should) get away with reusing existing types such as <code>sense</code> and <code>pos</code>. If you do add a new tag type, you should add it to <code>@TextTagger::all_tag_types</code> in <code>TextTagger.pm</code>, and you'll probably need to handle it somehow in <code>CombineTags.pm</code> and/or <code>Tags2Trips.pm</code>.</p>

<div class="section">
<h4>Tagger description fields</h4>
<p>The hashref describing your tagger can have the following fields:</p>
<dl>
 <dt><code>name</code> (required)</dt>
  <dd>The name of your tagger, lowercase with underscores, e.g. <code>"mome_rath"</code>.</dd>
 <dt><code>tag_function</code> (required)</dt>
  <dd>The function to be called for each tag request that selects your tagger. Its arguments will be the TextTagger object (usually called <code>$self</code>), the input text string (if <code>input_text =&gt; 1</code> is specified), and the list (not listref) of tags that have been found so far (if <code>input_types</code> is specified). It should return the listref (not list) of tags found by your tagger.</dd>
 <dt><code>output_types</code> (required)</dt>
  <dd>A listref of the types of tags your tagger is capable of outputting. If your tagger is capable of outputting any type of tag, use <code>[@TextTagger::all_tag_types]</code>.</dd>
 <dt><code>init_function</code> (optional)</dt>
  <dd>The function to be called to initialize your tagger. Normally this will happen when TextTagger starts up, and your tagger is listed in an <a href="#sec-3.1.2."><code>-init-taggers</code> option</a>. It can also be called in response to an <a href="#sec-3.3."><code>init</code> or <code>re-init</code> request</a>. Its only argument will be <code>$self</code> (the TextTagger object).</dd>
 <dt><code>fini_function</code> (optional)</dt>
  <dd>The function to be called to finalize your tagger. Normally this will happen when TextTagger exits, and your tagger was previously initialized. Only taggers with <code>init_function</code>s may also have <code>fini_function</code>s. Your <code>fini_function</code> can also be called in response to a <a href="#sec-3.1.2."><code>fini</code> or <code>re-init</code> request</a>. Its only argument will be <code>$self</code> (the TextTagger object).</dd>
 <dt><code>ready_function</code> (optional)</dt>
  <dd>The function to be called to wait for your tagger to finish initializing. You can make your <code>init_function</code> merely start the initialization process in parallel with other taggers (e.g. by spawning an external process), and this function will be called after all taggers have started initializing but before any tag requests are processed. This parallelization can decrease TextTagger's overall startup time. It is optional, though; you may define an <code>init_function</code> without defining a <code>ready_function</code>, in which case your tagger will be considered ready when your <code>init_function</code> returns. Note that the return value of the <code>ready_function</code> is not used, only the timing of when it returns. Its only argument will be <code>$self</code> (the TextTagger object).</dd>
 <dt><code>input_text</code> (optional)</dt>
  <dd>A boolean that indicates whether your tagger uses the raw input text string, as opposed to using tags from other taggers as input. The default is false. Using both the input text and input tags is allowed; the text will be passed first.</dd>
 <dt><code>input_types</code> (optional)</dt>
  <dd>A listref of the types of tags your tagger needs as input. Listing a type here will cause TextTagger to pull in other taggers that can output that type if none are already pulled in. Note that this is a list of <em>tag types</em>, not taggers; taggers cannot directly depend on each other.</dd>
 <dt><code>optional_input_types</code> (optional)</dt>
  <dd>A listref of the types of taggers your tagger can use as input, but doesn't strictly need. Listing type here will cause TextTagger to make an effort to run your tagger only after all taggers that can output that type have already run, but it won't cause any new taggers to be pulled in just for the purpose of getting that tag type. You may list the same type as both an input type and an optional output type; in this case your tagger is a "loopy" tagger, and will run after the non-loopy taggers outputting that type. But it might or might not run after other loopy taggers for that type. Other kinds of cycles in the <a href="#sec-5.4.">dependency graph</a> are not allowed. (See also the last paragraph of the section on <a href="#sec-3.5.2."><code>:type</code></a>.)</dd>
</dl>

<p>Note that all the functions take <code>$self</code>, the TextTagger object, as their first argument. This is a <code>TripsModule</code> object, so you can use it to make requests of other TRIPS modules by calling the <code>send_and_wait</code> method. Some taggers also use <code>$self</code> to help propagate their parameters from <a href="#sec-3.1.">the command line</a>, <a href="#sec-3.2."><code>set-parameters</code> requests</a>, or <a href="#sec-3.5."><code>tag</code> requests</a>, but currently this is kind of ugly and requires cooperation in <code>TextTagger.pm</code> itself.</p>
</div>

<div class="section">
<h4>Tagger template</h4>
<p>In general, if you were to write a tagger Perl module called <code>MomeRaths.pm</code>, it should look something like this. Please do not cargo-cult copy-and-paste everything from this template, as some of it surely doesn't apply to your tagger. Update the comments, at least.</p>
<blockquote><samp>
package TextTagger::MomeRaths;
require Exporter;
@ISA = qw(Exporter);
@EXPORT_OK = qw(init_mome_raths ready_mome_raths fini_mome_raths tag_mome_raths);

sub init_mome_raths {
  my $self = shift;
  # initialization code goes here, e.g. loading data, starting external processes
}

sub ready_mome_raths {
  my $self = shift;
  # do things like waiting for an external process to finish its initialization here
}

sub fini_mome_raths {
  my $self = shift;
  # finalization code goes here, e.g. stopping external processes
}

sub tag_mome_raths {
  # for taggers that only need text input:
  # my ($self, $input_text) = @_;
  # for taggers that only need tag input:
  # my ($self, @input_tags) = @_;
  # for taggers that use both:
  my ($self, $input_text, @input_tags) = @_;

  my @output_tags = ();

  # MAIN TAGGING CODE GOES HERE

  # adding an example POS tag:
  push @output_tags, {
    type =&gt; 'pos',
    start =&gt; $start,
    end =&gt; $end,
    lex =&gt; substr($input_text, $start, $end - $start),
    'penn-pos' =&gt; ['NN', 'NNP'] # noun or proper noun (singular)
  };

  # a common pattern is to run a regex over the $input_text and tag the matches;
  # the match2tag function in Util.pm helps with this by making the
  # start/end/lex fields for you from the most recent match:
  while ($input_text =~ /your regex here/g) {
    push @output_tags, +{
      type =&gt; 'sense',
      lftype =&gt; ['GEOGRAPHIC-REGION'], # a list of ONT types
      match2tag()
    };
  }

  return [@output_tags];
}

push @TextTagger::taggers, {
  # most of these fields are optional
  name =&gt; 'mome_raths', # required
  init_function =&gt; \&amp;init_mome_raths,
  ready_function =&gt; \&amp;ready_mome_raths,
  fini_function =&gt; \&amp;fini_mome_raths,
  tag_function =&gt; \&amp;tag_mome_raths, # required
  output_types =&gt; [qw(pos sense)], # required
  input_text =&gt; 1,
  input_types =&gt; ['sentence'],
  optional_input_types =&gt; ['clause']
};

1;
</samp></blockquote>

</div>

</div>

</div>

<div class="section">
<h2>Stanford weirdness</h2>
<p>The old separate tools from Stanford that TextTagger uses (their NER, POS tagger, and parser) do some weird things that make it difficult for TextTagger to deal with them (in particular, matching up their output with the original input string is a problem). Most of these things are done only by the NER and parser, not by the POS tagger (which is weird in itself). The POS tagger does insist that its input is spaced in a particular way, but that's about it. The rest of these apply to the NER and parser:</p>
<ul>
<li>They convert most kinds of braces ( ([&lt; and &gt;]) ) to -LRB- and -RRB-, which are the Penn tags for Left Round Brace and Right Round Brace, which is wrong because [&lt;&gt;] are not round.</li>
<li>They convert curly braces { and } to -LCB- and -RCB-, which are correct according to Penn, but why pick these out from all the other braces they say are "round"?</li>
<li>Stanford Parser sometimes <em>doesn't</em> convert ( and ) to -LRB- and -RRB-, making the Lispy parse trees it outputs more difficult to interpret.</li>
<li>They convert ". . ." to "..." (removing spaces), and convert "…" to "..." (Unicode to ASCII).</li>
<li>They convert " to `` or '', if they think it's the start or end of a quotation, respectively.</li>
<li>They convert ' to ` in similar situations.</li>
<li>They convert Unicode quote marks (except low ones and high-reversed-9 ones) to ASCII equivalents using backticks ` and apostrophes ', including right single quotes used as apostrophes in endings like "n't"/"n’t".</li>
<li>They convert Unicode en-dashes (–) and em-dashes (—) to double ASCII hyphen-minuses (--).</li>
<li>They insert an extra "." after an abbreviation if they think it's at the end of a sentence, e.g. "...Hewlett-Packard Co. Recently..." becomes "...Hewlett-Packard Co. . Recently...".</li>
<li>They convert (some) British spellings like "organisation" and "colour" to their American equivalents like "organization" and "color".</li>
<li>They capitalize days of the week, and months of the year except "may".</li>
<li>They convert the cents character "¢" to the word "cents".</li>
<li>Stanford NER doesn't tag things that look like XML tags; it passes them through unchanged.</li>
<li>...Except when they occur at the beginning of the string. If there is a sequence of XML tags at the beginning, only the last one is kept (but still not tagged), and any space between it and the following token will be removed, e.g. "&lt;foo&gt; &lt;bar&gt; &lt;baz&gt; test" becomes " &lt;baz&gt;test". This doesn't appear to affect the tagging of the following token, e.g. in "&lt;foo&gt; Mount Rushmore" -&gt; "&lt;foo&gt;Mount Rushmore", "Mount Rushmore" is still tagged as a <code>LOCATION</code>.</li>
<li>Some, but by no means all, HTML entities are converted: "&amp;mdash;" becomes "--", "&amp;quot;" becomes "``" or "''" depending on position, and "&amp;apos;" becomes "'" unconditionally (never "`"). Notably, "&amp;cent;" isn't converted to "cents" the way the literal cents character is, neither is "&amp;ndash;" converted to "--".</li>
</ul>
<p>There may be more things like this lurking. In particular, I doubt I covered all their British to American spelling mappings (though I already know they didn't do all of them). If you see the error message "word not found in remaining original text", it's likely something like this is the problem.</p>
<p>The new integrated Stanford CoreNLP still does a lot of this stuff, but it's not really a problem because it also now gives TextTagger the character offsets for the token boundaries, so it's no longer necessary to match up the output string to the original input. One thing that still happens is that sentence-ending abbreviations get their final periods chopped off to serve as the end of the sentence (so in the above example you'd get "Co" and "." in separate tags, but you wouldn't get "Co."). </p>
</div>

</div>
</body></html>

